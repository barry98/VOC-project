{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\tqdm\\std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import notebook, trange, tqdm\n",
    "import spacy\n",
    "import ast\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import itertools\n",
    "import logging\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "import argparse\n",
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "spacy.require_gpu()\n",
    "notebook.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(ner, true, distance):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    check = []\n",
    "    corrector = []\n",
    "    for x in true:\n",
    "        # Check Normal Names\n",
    "        if x in ner:\n",
    "            tp += 1\n",
    "            corrector.append(x)\n",
    "            continue \n",
    "\n",
    "        # Check Removed Last Names \n",
    "        for y in ner:\n",
    "            if type(y) == tuple:\n",
    "                if y[0] == x:\n",
    "                    tp += 1\n",
    "                    corrector.append(y[0])\n",
    "                    break\n",
    "                elif y[1] == x:\n",
    "                    tp += 1\n",
    "                    corrector.append(y[1])\n",
    "                    break\n",
    "                else:\n",
    "                    if fuzz.ratio(x,y[0]) >= distance:\n",
    "                        tp += 1\n",
    "                        check.append((x,y[1]))\n",
    "                        corrector.append(x)\n",
    "                        break\n",
    "                    elif fuzz.ratio(x[1],y) >= distance:\n",
    "                        tp += 1\n",
    "                        check.append((x[0],y))\n",
    "                        corrector.append(x)\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                if fuzz.ratio(x.lower(),y.lower()) >= distance:\n",
    "                    tp += 1\n",
    "                    corrector.append(x)\n",
    "                    check.append((x,y))\n",
    "                    break\n",
    "            \n",
    "        if x not in corrector:\n",
    "            fn += 1\n",
    "    fp = len(ner) - tp\n",
    "    return tp, fp, fn, check\n",
    "\n",
    "def spacy_name_getter(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    holder = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON' and ' ' in ent.text:\n",
    "#             if re.search(\"[A-Z][a-z ]+[A-Z]\\w+ [A-Z]\\w+\", ent.text) != None:\n",
    "#                 holder.append((ent.text, re.sub(' [A-Z]\\w+$', '', ent.text)))\n",
    "#             else:\n",
    "            holder.append(ent.text)\n",
    "    return holder\n",
    "\n",
    "def true_name_getter(true):\n",
    "    holder = []\n",
    "    for x in ast.literal_eval(true):\n",
    "        if x['tussenvoegsel'] != None:\n",
    "            holder.append(x['voornaam'] + \" \" + x['tussenvoegsel'] + \" \" + x['achternaam'])\n",
    "        elif x['voornaam'] and x['achternaam'] != None:\n",
    "            holder.append(x['voornaam'] + \" \" + x['achternaam'])\n",
    "    return holder\n",
    "\n",
    "def NER_test(row, model, levenshtein, nlp):\n",
    "    if model == 'SpaCy':\n",
    "        ner = spacy_name_getter(row.text, nlp)\n",
    "    if model == 'BERTje':\n",
    "        ner = bertje_test(row.ner_clean)\n",
    "    true = true_name_getter(row.namen)\n",
    "    tp, fp, fn, check = compare(ner, true, levenshtein)\n",
    "    #return {'tp' : tp, 'fp': fp, 'fn': fn}\n",
    "    return {'tp' : tp, 'fp': fp, 'fn': fn, 'check': check}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
    "existing model or a blank model.\n",
    "\n",
    "For more details, see the documentation:\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "Last tested with: v2.1.0\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @plac.annotations(\n",
    "#     model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "#     output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "#     n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    "# )\n",
    "def main(train=None, model=None, output_dir=None, n_iter=100, dropout=0.5, start=4.0, end=32.0, compound=1.001, test=None):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    TRAIN_DATA = train\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model, disable=['parser', 'tagger', 'textcat'])  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly â€“ but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in notebook.tqdm(range(n_iter)):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(start, end, compound))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=dropout,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            #print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    #for text, _ in TRAIN_DATA:\n",
    "        #doc = nlp(text)\n",
    "        #print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        #print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        if type(test) != type(None):\n",
    "            print(\"Loading from\", output_dir)\n",
    "            nlp2 = spacy.load(output_dir, disable=['parser', 'tagger', 'textcat'])\n",
    "            ner_result = test.progress_apply(NER_test, args=('SpaCy', 90, nlp2), axis=1)\n",
    "            total_fp = sum([x['fp'] for x in ner_result])\n",
    "            total_tp = sum([x['tp'] for x in ner_result])\n",
    "            total_fn = sum([x['fn'] for x in ner_result])\n",
    "            print('Recall: ' + str(total_tp / (total_tp + total_fn)))\n",
    "            print('Precision: ' + str(total_tp / (total_tp + total_fp)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../kfold_6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097f120a82da4e128835a43817369b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall: 0.7421602787456446\n",
      "Precision: 0.5545619198551053\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_json('train_kfold_6.json').traindata.tolist()\n",
    "test = pd.read_json('test_kfold6.json')\n",
    "#main(train=train, model='nl_core_news_sm', output_dir='../kfold_' + '6', dropout=0.5, test=test)\n",
    "if type(test) != type(None):\n",
    "            print(\"Loading from\", '../kfold_' + '6')\n",
    "            nlp2 = spacy.load('../kfold_' + '6', disable=['parser', 'tagger', 'textcat'])\n",
    "            ner_result = test.progress_apply(NER_test, args=('SpaCy', 90, nlp2), axis=1)\n",
    "            total_fp = sum([x['fp'] for x in ner_result])\n",
    "            total_tp = sum([x['tp'] for x in ner_result])\n",
    "            total_fn = sum([x['fn'] for x in ner_result])\n",
    "            print('Recall: ' + str(total_tp / (total_tp + total_fn)))\n",
    "            print('Precision: ' + str(total_tp / (total_tp + total_fp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'nl_core_news_sm'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e12d844cdc54393933532cd47f9d967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_json('train_kfold_7.json').traindata.tolist()\n",
    "test = pd.read_json('test_kfold7.json')\n",
    "main(train=train, model='nl_core_news_sm', output_dir='../kfold_' + '7', dropout=0.5, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('train_kfold_8.json').traindata.tolist()\n",
    "test = pd.read_json('test_kfold8.json')\n",
    "main(train=train, model='nl_core_news_sm', output_dir='../kfold_' + '8', dropout=0.5, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('train_kfold_9.json').traindata.tolist()\n",
    "test = pd.read_json('test_kfold9.json')\n",
    "main(train=train, model='nl_core_news_sm', output_dir='../kfold_' + '9', dropout=0.5, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('train_kfold_10.json').traindata.tolist()\n",
    "test = pd.read_json('test_kfold10.json')\n",
    "main(train=train, model='nl_core_news_sm', output_dir='../kfold_' + '10', dropout=0.5, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
