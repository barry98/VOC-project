{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import notebook, trange\n",
    "import spacy\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertModel, AutoModelWithLMHead, AutoTokenizer\n",
    "#from transformers import RobertaTokenizer, RobertaForSequenceClassification, pipeline\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertAdam\n",
    "from nltk.corpus import conll2002\n",
    "import itertools\n",
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1060 6GB'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\tqdm\\std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../clean_data.csv')\n",
    "notebook.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download nl_core_news_sm\n",
    "nlp = spacy.load('nl_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_ratio_and_distance(s, t, ratio_calc = False):\n",
    "    \"\"\" levenshtein_ratio_and_distance:\n",
    "        Calculates levenshtein distance between two strings.\n",
    "        If ratio_calc = True, the function computes the\n",
    "        levenshtein distance ratio of similarity between two strings\n",
    "        For all i and j, distance[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "    \"\"\"\n",
    "    # Initialize matrix of zeros\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    distance = np.zeros((rows,cols),dtype = int)\n",
    "\n",
    "    # Populate matrix of zeros with the indeces of each character of both strings\n",
    "    for i in range(1, rows):\n",
    "        for k in range(1,cols):\n",
    "            distance[i][0] = i\n",
    "            distance[0][k] = k\n",
    "\n",
    "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
    "            else:\n",
    "                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\n",
    "                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\n",
    "                if ratio_calc == True:\n",
    "                    cost = 2\n",
    "                else:\n",
    "                    cost = 1\n",
    "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\n",
    "                                 distance[row][col-1] + 1,          # Cost of insertions\n",
    "                                 distance[row-1][col-1] + cost)     # Cost of substitutions\n",
    "    if ratio_calc == True:\n",
    "        # Computation of the Levenshtein Distance Ratio\n",
    "        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))\n",
    "        return Ratio\n",
    "    else:\n",
    "        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,\n",
    "        # insertions and/or substitutions\n",
    "        # This is the minimum number of edits needed to convert string a to string b\n",
    "        return distance[row][col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(ner, true, distance):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    check = []\n",
    "    corrector = []\n",
    "    for x in ner:\n",
    "        # Check Removed Last Names\n",
    "        if type(x) == tuple:\n",
    "            if x[0] in true:\n",
    "                tp += 1\n",
    "                corrector.append(x[0])\n",
    "            elif x[1] in true:\n",
    "                tp += 1\n",
    "                corrector.append(x[1])\n",
    "            else:\n",
    "                switch = True\n",
    "                for y in true:\n",
    "                    if levenshtein_ratio_and_distance(x[0].lower(),y.lower()) <= distance:\n",
    "                        tp += 1\n",
    "                        switch = False\n",
    "                        check.append((x[0],y))\n",
    "                        corrector.append(y)\n",
    "                    elif levenshtein_ratio_and_distance(x[1].lower(),y.lower()) <= distance:\n",
    "                        tp += 1\n",
    "                        switch = False\n",
    "                        check.append((x[1],y))\n",
    "                        corrector.append(y)\n",
    "                if switch == True:\n",
    "                    fp += 1\n",
    "        \n",
    "        # Check Normal Names\n",
    "        elif x in true:\n",
    "            tp += 1\n",
    "            corrector.append(x)\n",
    "        else:\n",
    "            switch = True\n",
    "            for y in true:\n",
    "                if levenshtein_ratio_and_distance(x.lower(),y.lower()) <= distance:\n",
    "                    tp += 1\n",
    "                    switch = False\n",
    "                    corrector.append(y)\n",
    "                    check.append((x,y))\n",
    "            if switch == True:\n",
    "                fp += 1\n",
    "            \n",
    "    for x in true:\n",
    "        if x not in corrector:\n",
    "            fn += 1\n",
    "    return tp, fp, fn, check\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_test(text):\n",
    "    doc = nlp(text)\n",
    "    holder = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER' and ' ' in ent.text:\n",
    "            if re.search(\"[A-Z][a-z ]+[A-Z]\\w+ [A-Z]\\w+\", ent.text) != None:\n",
    "                holder.append((ent.text, re.sub(' [A-Z]\\w+$', '', ent.text)))\n",
    "            else:\n",
    "                holder.append(ent.text)\n",
    "    return holder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertje_test(ner_clean):\n",
    "    holder = []\n",
    "    prev = None\n",
    "    for x in ner_clean:\n",
    "        if x[-1].strip() == 'B-PER':\n",
    "            holder.append(x[0])\n",
    "            prev = x[-1].strip()\n",
    "        elif x[-1].strip() == 'I-PER':\n",
    "            if prev == 'B-PER' or prev == 'I-PER':\n",
    "                holder[-1] += ' ' + x[0]\n",
    "                prev = x[-1].strip()\n",
    "            else:\n",
    "                holder.append(x[0])\n",
    "                prev = x[-1].strip()\n",
    "        else:\n",
    "            prev = x[-1].strip()\n",
    "        \n",
    "    return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_name_getter(true):\n",
    "    holder = []\n",
    "    for x in ast.literal_eval(true):\n",
    "        if x['tussenvoegsel'] != None:\n",
    "            holder.append(x['voornaam'] + \" \" + x['tussenvoegsel'] + \" \" + x['achternaam'])\n",
    "        elif x['voornaam'] and x['achternaam'] != None:\n",
    "            holder.append(x['voornaam'] + \" \" + x['achternaam'])\n",
    "    return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER_test(row, model, levenshtein):\n",
    "    if model == 'SpaCy':\n",
    "        ner = spacy_test(row.text)\n",
    "    if model == 'BERTje':\n",
    "        ner = bertje_test(row.ner_clean)\n",
    "    if model == 'ROBERTA':\n",
    "        ner = roberta_test\n",
    "    true = true_name_getter(row.namen)\n",
    "    tp, fp, fn, check = compare(ner, true, levenshtein)\n",
    "    #return {'tp' : tp, 'fp': fp, 'fn': fn}\n",
    "    return {'tp' : tp, 'fp': fp, 'fn': fn, 'check': check}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_result = df.progress_apply(NER_test, args=('SpaCy'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fp = sum([x['fp'] for x in ner_result])\n",
    "total_tp = sum([x['tp'] for x in ner_result])\n",
    "total_fn = sum([x['fn'] for x in ner_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fp,total_tp,total_fn, total_tp / (total_tp + total_fp), total_tp / (total_tp + total_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Tests\n",
    "\n",
    "#### Base SpaCy\n",
    "TP: 23862  \n",
    "FP: 190907  \n",
    "FN: 45972  \n",
    "Recall: 0.34169602199501675  \n",
    "Precision: 0.11110542024221373  \n",
    "\n",
    "#### SpaCy met achternaam removal\n",
    "TP: 26937  \n",
    "FP: 187832  \n",
    "FN: 45074  \n",
    "Recall: 0.3740678507450251  \n",
    "Precision: 0.12542312903631345  \n",
    "\n",
    "#### SpaCy met Levenshtein distance 2 en achternaam removal\n",
    "TP: 41931  \n",
    "FP: 173001  \n",
    "FN: 34235  \n",
    "Recall: 0.5505212299451199  \n",
    "Precision: 0.1950896097370331  \n",
    "\n",
    "#### SpaCy met Levenshtein distance 3 en achternaam removal\n",
    "TP: 47121  \n",
    "FP: 167997  \n",
    "FN: 30955  \n",
    "Recall: 0.6035273323428454  \n",
    "Precision: 0.2190472205952082  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_test(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, max_len, prev, counter, hold_list, tokenizer):\n",
    "    holder = hold_list\n",
    "    if len(data[prev:]) == 0:\n",
    "        return holder\n",
    "    \n",
    "    data_text = data[prev:]\n",
    "    subword_len = []\n",
    "    line = data_text.rstrip()\n",
    "    line = line.split(' ')\n",
    "    if len(line) == 1:\n",
    "        text = '\\n'.join(line)\n",
    "        holder.append(text)\n",
    "        return holder\n",
    "    subword_len_counter = 0\n",
    "    \n",
    "    for x in line:\n",
    "        current_subwords_len = len(''.join(tokenizer.tokenize(x)))\n",
    "        if (subword_len_counter + current_subwords_len) > max_len:\n",
    "            text = '\\n'.join(subword_len)\n",
    "            holder.append(text)\n",
    "            return preprocess(data, max_len, prev + len(text), counter + 1, holder, tokenizer)\n",
    "        else:\n",
    "            subword_len.append(x)\n",
    "            subword_len_counter += current_subwords_len\n",
    "    \n",
    "    text = '\\n'.join(subword_len)    \n",
    "    holder.append(text)\n",
    "\n",
    "        \n",
    "    return preprocess(data, max_len, prev + len(text), counter + 1, holder, tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Named entity recognition fine-tuning: utilities to work with CoNLL-2003 task. \"\"\"\n",
    "\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "\n",
    "def read_examples_from_file(data_dir, mode):\n",
    "    guid_index = 1\n",
    "    examples = []\n",
    "    \n",
    "    words = []\n",
    "    labels = []\n",
    "    for line in mode.split('\\n'):\n",
    "        if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "            if words:\n",
    "                examples.append(InputExample(guid=\"{}-{}\".format(mode, guid_index), words=words, labels=labels))\n",
    "                guid_index += 1\n",
    "                words = []\n",
    "                labels = []\n",
    "        else:\n",
    "            splits = line.split(\" \")\n",
    "            words.append(splits[0])\n",
    "            if len(splits) > 1:\n",
    "                labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
    "            else:\n",
    "                # Examples could have no label for mode = \"test\"\n",
    "                labels.append(\"O\")\n",
    "    if words:\n",
    "        examples.append(InputExample(guid=\"{}-{}\".format(mode, guid_index), words=words, labels=labels))\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(\n",
    "    examples,\n",
    "    label_list,\n",
    "    max_seq_length,\n",
    "    tokenizer,\n",
    "    cls_token_at_end=False,\n",
    "    cls_token=\"[CLS]\",\n",
    "    cls_token_segment_id=1,\n",
    "    sep_token=\"[SEP]\",\n",
    "    sep_token_extra=False,\n",
    "    pad_on_left=False,\n",
    "    pad_token=0,\n",
    "    pad_token_segment_id=0,\n",
    "    pad_token_label_id=-100,\n",
    "    sequence_a_segment_id=0,\n",
    "    mask_padding_with_zero=True,\n",
    "):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n",
    "\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "            # bert-base-multilingual-cased sometimes output \"nothing ([]) when calling tokenize with just a space.\n",
    "            if len(word_tokens) > 0:\n",
    "                tokens.extend(word_tokens)\n",
    "                # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "                label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = tokenizer.num_added_tokens()\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids:   0   0   0   0  0     0   0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        if sep_token_extra:\n",
    "            # roberta uses an extra separator b/w pairs of sentences\n",
    "            tokens += [sep_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens += [cls_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "            segment_ids += [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            label_ids = [pad_token_label_id] + label_ids\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n",
    "        else:\n",
    "            input_ids += [pad_token] * padding_length\n",
    "            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
    "            segment_ids += [pad_token_segment_id] * padding_length\n",
    "            label_ids += [pad_token_label_id] * padding_length\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\", example.guid)\n",
    "            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids)\n",
    "        )\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_labels(path):\n",
    "    if path:\n",
    "        with open(path, \"r\") as f:\n",
    "            labels = f.read().splitlines()\n",
    "        if \"O\" not in labels:\n",
    "            labels = [\"O\"] + labels\n",
    "        return labels\n",
    "    else:\n",
    "        return [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\" Fine-tuning the library models for named entity recognition on CoNLL-2003 (Bert or Roberta). \"\"\"\n",
    "\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "#logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in MODEL_CONFIG_CLASSES), ())\n",
    "\n",
    "TOKENIZER_ARGS = [\"do_lower_case\", \"strip_accents\", \"keep_accents\", \"use_fast\"]\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "def evaluate(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=\"\"):\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "#     logger.info(\"***** Running evaluation %s *****\", prefix)\n",
    "#     logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "#     logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = (\n",
    "                    batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "                )  # XLM and RoBERTa don\"t use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n",
    "\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "    for i in range(out_label_ids.shape[0]):\n",
    "        for j in range(out_label_ids.shape[1]):\n",
    "            if out_label_ids[i, j] != pad_token_label_id:\n",
    "                out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "    results = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list),\n",
    "    }\n",
    "\n",
    "    return results, preds_list\n",
    "\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode):\n",
    "    if args.local_rank not in [-1, 0] and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            mode, list(filter(None, args.model_name_or_path.split(\"/\"))).pop(), str(args.max_seq_length)\n",
    "        ),\n",
    "    )\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        examples = read_examples_from_file(args.data_dir, mode)\n",
    "        features = convert_examples_to_features(\n",
    "            examples,\n",
    "            labels,\n",
    "            args.max_seq_length,\n",
    "            tokenizer,\n",
    "            cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n",
    "            # xlnet has a cls token at the end\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            sep_token_extra=bool(args.model_type in [\"roberta\"]),\n",
    "            # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "            pad_on_left=bool(args.model_type in [\"xlnet\"]),\n",
    "            # pad on the left for xlnet\n",
    "            pad_token=tokenizer.pad_token_id,\n",
    "            pad_token_segment_id=tokenizer.pad_token_type_id,\n",
    "            pad_token_label_id=pad_token_label_id,\n",
    "        )\n",
    "#         if args.local_rank in [-1, 0]:\n",
    "#             logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "#             torch.save(features, cached_features_file)\n",
    "\n",
    "    if args.local_rank == 0 and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def main(files=None, data_dir=None, model_type=None, model_name_or_path=None, output_dir=None,\n",
    "         max_seq_length=128, seed=42, do_predict=False, labels=\"\", config_name=\"\", \n",
    "         tokenizer_name=\"\", cache_dir=\"\", do_train=False, do_eval=False, \n",
    "         evaluate_during_training=False, do_lower_case=False, keep_accents=False,\n",
    "         strip_accents=False, use_fast=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8,\n",
    "         gradient_accumulation_steps=1, learning_rate=5e-5, weight_decay=0.0, adam_epsilon=1e-8,\n",
    "         max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_steps=500,\n",
    "         save_steps=500, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False,\n",
    "         overwrite_cache=False, fp16=False, fp16_opt_level=\"O1\", local_rank=-1):\n",
    "    \n",
    "#     parser = argparse.ArgumentParser()\n",
    "    args = argparse.Namespace(data_dir=data_dir, model_type=model_type, model_name_or_path=model_name_or_path,\n",
    "                               labels=labels, config_name=config_name, tokenizer_name=tokenizer_name,\n",
    "                               cache_dir=cache_dir, max_seq_length=max_seq_length, do_train=do_train,\n",
    "                               do_eval=do_eval, do_predict=do_predict, evaluate_during_training=evaluate_during_training,\n",
    "                               do_lower_case=do_lower_case, keep_accents=keep_accents, strip_accents=strip_accents,\n",
    "                               use_fast=use_fast, per_gpu_train_batch_size=per_gpu_train_batch_size,\n",
    "                               per_gpu_eval_batch_size=per_gpu_eval_batch_size, \n",
    "                               gradient_accumulation_steps=gradient_accumulation_steps, learning_rate=learning_rate,\n",
    "                               weight_decay=weight_decay, adam_epsilon=adam_epsilon, max_grad_norm=max_grad_norm, \n",
    "                               num_train_epochs=num_train_epochs, max_steps=max_steps, warmup_steps=warmup_steps,\n",
    "                               logging_steps=logging_steps, ave_steps=save_steps, eval_all_checkpoints=eval_all_checkpoints,\n",
    "                               no_cuda=no_cuda, overwrite_output_dir=overwrite_output_dir, overwrite_cache=overwrite_cache,\n",
    "                               seed=seed, fp16=fp16, fp16_opt_level=fp16_opt_level, local_rank=local_rank,\n",
    "                               output_dir=output_dir, files=files)\n",
    "\n",
    "#     if (\n",
    "#         os.path.exists(args.output_dir)\n",
    "#         and os.listdir(args.output_dir)\n",
    "#         and args.do_train\n",
    "#         and not args.overwrite_output_dir\n",
    "#     ):\n",
    "#         raise ValueError(\n",
    "#             \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "#                 args.output_dir\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if local_rank == -1 or no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        device = torch.device(\"cuda\", local_rank)\n",
    "        torch.distributed.init_process_group(backend=\"nccl\")\n",
    "        args.n_gpu = 1\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "#     logging.basicConfig(\n",
    "#         format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "#         datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "#         level=logging.INFO if local_rank in [-1, 0] else logging.WARN,\n",
    "#     )\n",
    "#     logger.warning(\n",
    "#         \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "#         local_rank,\n",
    "#         device,\n",
    "#         n_gpu,\n",
    "#         bool(args.local_rank != -1),\n",
    "#         fp16,\n",
    "#     )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    # Prepare CONLL-2003 task\n",
    "    labels = get_labels(labels)\n",
    "    num_labels = len(labels)\n",
    "    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "    pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    if local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    model_type = model_type.lower()\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        config_name if config_name else model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        id2label={str(i): label for i, label in enumerate(labels)},\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    "        cache_dir=cache_dir if cache_dir else None,\n",
    "    )\n",
    "    tokenizer_args = {k: v for k, v in vars(args).items() if v is not None and k in TOKENIZER_ARGS}\n",
    "#     logger.info(\"Tokenizer arguments: %s\", tokenizer_args)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "        **tokenizer_args,\n",
    "    )\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    model.to(args.device)\n",
    "\n",
    "#     logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=\"train\")\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer, labels, pad_token_label_id)\n",
    "#         logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "        # Create output directory if needed\n",
    "        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "            os.makedirs(args.output_dir)\n",
    "\n",
    "#         logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir, **tokenizer_args)\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            model = AutoModelForTokenClassification.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result, _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=\"dev\", prefix=global_step)\n",
    "            if global_step:\n",
    "                result = {\"{}_{}\".format(global_step, k): v for k, v in result.items()}\n",
    "            results.update(result)\n",
    "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            for key in sorted(results.keys()):\n",
    "                writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
    "    \n",
    "    if args.do_predict and args.local_rank in [-1, 0]:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir, **tokenizer_args)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "        def applier(row, args, tokenizer):\n",
    "            final_predictions = []\n",
    "            for file in preprocess(row.text, 100, 0, 1, [], tokenizer):\n",
    "                result, predictions = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=file)\n",
    "\n",
    "                # Save results\n",
    "                #final_predictions.append((file, predictions))\n",
    "\n",
    "                # Save predictions\n",
    "\n",
    "                example_id = 0\n",
    "                for line in file.split('\\n'):\n",
    "                    if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                        final_predictions.append(line)\n",
    "                        if not predictions[example_id]:\n",
    "                            example_id += 1\n",
    "                    elif predictions[example_id]:\n",
    "                        output_line = line.split()[0] + \" \" + predictions[example_id].pop(0) + \"\\n\"\n",
    "                        final_predictions.append(output_line)\n",
    "#                     else:\n",
    "#                         logger.warning(\"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0])\n",
    "\n",
    "            return final_predictions\n",
    "        return files.progress_apply(applier, args=(args, tokenizer), axis=1)\n",
    "    else:\n",
    "        return 'spooples'\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7360d1f510f14d01bf5b9cb36db930e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13063), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_ner = main(files=df, data_dir=\"D:\\Documenten\\Studie\\Master\\Scriptie\\\\notaris_bert\", model_type=\"bert\", model_name_or_path='bert-base-dutch-cased',\n",
    "     output_dir='D:\\Documenten\\Studie\\Master\\Scriptie\\\\notaris_bert\\output', max_seq_length=128, save_steps=750, seed=1, do_predict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('N:', 'O\\n'),\n",
       " ('4', 'O\\n'),\n",
       " ('1e', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Testateuren', 'B-MISC\\n'),\n",
       " ('hebben', 'O\\n'),\n",
       " ('verklaerd', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('Samen', 'O\\n'),\n",
       " ('beneden', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('vierduijsend', 'O\\n'),\n",
       " ('guldens', 'O\\n'),\n",
       " ('', ''),\n",
       " ('gegold', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('Sijn', 'B-LOC\\n'),\n",
       " ('Teltamens', 'I-MISC\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('Martinus', 'B-PER\\n'),\n",
       " ('Ciprianus', 'I-PER\\n'),\n",
       " ('&', 'O\\n'),\n",
       " ('Margareta', 'B-PER\\n'),\n",
       " ('van', 'I-PER\\n'),\n",
       " ('Zuijderwijk', 'I-PER\\n'),\n",
       " ('', ''),\n",
       " ('Egtelieden', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('dato', 'O\\n'),\n",
       " ('16', 'O\\n'),\n",
       " ('Maert', 'O\\n'),\n",
       " ('1741tn', 'O\\n'),\n",
       " ('den', 'O\\n'),\n",
       " ('Name', 'O\\n'),\n",
       " ('Godes', 'I-PER\\n'),\n",
       " ('amen', 'O\\n'),\n",
       " ('op', 'O\\n'),\n",
       " ('heden', 'O\\n'),\n",
       " ('den', 'O\\n'),\n",
       " ('16:', 'O\\n'),\n",
       " ('maert', 'O\\n'),\n",
       " ('', ''),\n",
       " ('des', 'B-LOC\\n'),\n",
       " ('Jaers', 'I-LOC\\n'),\n",
       " ('1741', 'O\\n'),\n",
       " ('E', 'O\\n'),\n",
       " ('des', 'O\\n'),\n",
       " ('avonds', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('klocke', 'O\\n'),\n",
       " ('over', 'O\\n'),\n",
       " ('Seven', 'B-MISC\\n'),\n",
       " ('uuren', 'O\\n'),\n",
       " ('Compareerden', 'O\\n'),\n",
       " ('voor', 'O\\n'),\n",
       " ('mij', 'O\\n'),\n",
       " ('Jan', 'B-PER\\n'),\n",
       " ('herleid', 'I-PER\\n'),\n",
       " ('Notaris', 'I-PER\\n'),\n",
       " ('', ''),\n",
       " ('bij', 'O\\n'),\n",
       " ('den', 'O\\n'),\n",
       " ('Ed:', 'B-PER\\n'),\n",
       " ('hove', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('holland', 'O\\n'),\n",
       " ('geadmitteert', 'O\\n'),\n",
       " ('tot', 'O\\n'),\n",
       " ('Amsterdam', 'B-LOC\\n'),\n",
       " ('resideerende26', 'O\\n'),\n",
       " ('maert', 'O\\n'),\n",
       " ('', ''),\n",
       " ('1745', 'O\\n'),\n",
       " ('gNo:', 'O\\n'),\n",
       " ('4', 'O\\n'),\n",
       " ('Mousr:', 'O\\n'),\n",
       " ('Mantinus', 'B-PER\\n'),\n",
       " ('Cijpriauus', 'I-PER\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('Juffr:', 'O\\n'),\n",
       " ('Margareta', 'B-PER\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('Zuijderwijk', 'B-LOC\\n'),\n",
       " ('', ''),\n",
       " ('Egtelieden', 'O\\n'),\n",
       " ('wonende', 'O\\n'),\n",
       " ('binnen', 'O\\n'),\n",
       " ('dese', 'O\\n'),\n",
       " ('Stad', 'O\\n'),\n",
       " ('op', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('haerlemmerdijk', 'O\\n'),\n",
       " ('tusschende', 'O\\n'),\n",
       " ('Dommerstract', 'B-LOC\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('', ''),\n",
       " ('het', 'O\\n'),\n",
       " ('haer', 'O\\n'),\n",
       " ('lemmerplijn', 'O\\n'),\n",
       " ('mij', 'O\\n'),\n",
       " ('Nots', 'O\\n'),\n",
       " ('bekend', 'O\\n'),\n",
       " ('gezond', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('Lighamen', 'B-LOC\\n'),\n",
       " ('hun', 'O\\n'),\n",
       " ('verstand', 'O\\n'),\n",
       " ('memorie', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('uijt', 'O\\n'),\n",
       " ('sprake', 'O\\n'),\n",
       " ('', ''),\n",
       " ('wel', 'O\\n'),\n",
       " ('hebbende', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('gebruijkende', 'O\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('uijtterlijk', 'O\\n'),\n",
       " ('bleek', 'O\\n'),\n",
       " ('dewelke', 'O\\n'),\n",
       " ('verklaerden', 'O\\n'),\n",
       " ('uijt', 'O\\n'),\n",
       " ('', ''),\n",
       " ('overden�', 'O\\n'),\n",
       " ('kinge', 'O\\n'),\n",
       " ('des', 'O\\n'),\n",
       " ('Doods', 'I-PER\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('rade', 'O\\n'),\n",
       " ('geworden', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('Sijn', 'B-LOC\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('hunne', 'O\\n'),\n",
       " ('tijdelijke', 'O\\n'),\n",
       " ('goederen', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('desponeeren', 'O\\n'),\n",
       " ('', ''),\n",
       " ('hebben', 'O\\n'),\n",
       " ('daeromme', 'O\\n'),\n",
       " ('uijt', 'O\\n'),\n",
       " ('vrijen', 'O\\n'),\n",
       " ('wille', 'O\\n'),\n",
       " ('gemaakt', 'O\\n'),\n",
       " ('hun', 'O\\n'),\n",
       " ('Testament', 'B-MISC\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('uijtterste', 'O\\n'),\n",
       " ('wille', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('manieren', 'O\\n'),\n",
       " ('', ''),\n",
       " ('naervolgende', 'O\\n'),\n",
       " ('Eerstelijk', 'B-MISC\\n'),\n",
       " ('verklaren', 'I-MISC\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('Testateuren', 'B-MISC\\n'),\n",
       " ('(na', 'O\\n'),\n",
       " ('Christelijke', 'B-MISC\\n'),\n",
       " ('recom', 'O\\n'),\n",
       " ('mandatie', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('Zielen', 'B-ORG\\n'),\n",
       " ('&', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Lighamen)', 'B-PER\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('revoieeren', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " (\"'t\", 'O\\n'),\n",
       " ('vernietigen', 'O\\n'),\n",
       " ('alle', 'O\\n'),\n",
       " ('voorgoende', 'O\\n'),\n",
       " ('Testamenten', 'B-MISC\\n'),\n",
       " ('Codicille', 'I-MISC\\n'),\n",
       " ('off', 'I-MISC\\n'),\n",
       " ('', ''),\n",
       " ('te', 'O\\n'),\n",
       " ('andre', 'O\\n'),\n",
       " ('uijtterste', 'O\\n'),\n",
       " ('willens', 'O\\n'),\n",
       " ('dispositien', 'O\\n'),\n",
       " ('door', 'O\\n'),\n",
       " ('hen', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('Sanmen', 'O\\n'),\n",
       " ('ider', 'O\\n'),\n",
       " ('apart', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('met', 'O\\n'),\n",
       " ('imand', 'O\\n'),\n",
       " ('anders', 'O\\n'),\n",
       " ('', ''),\n",
       " ('voor', 'O\\n'),\n",
       " ('dato', 'O\\n'),\n",
       " ('deses', 'O\\n'),\n",
       " ('gepas�', 'O\\n'),\n",
       " ('seert;', 'O\\n'),\n",
       " ('begeerende', 'O\\n'),\n",
       " ('dat', 'O\\n'),\n",
       " ('deselve', 'O\\n'),\n",
       " ('geen', 'O\\n'),\n",
       " ('kragt', 'O\\n'),\n",
       " ('hebben', 'O\\n'),\n",
       " ('offte', 'O\\n'),\n",
       " ('eenig', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Effect', 'O\\n'),\n",
       " ('sorteeren', 'O\\n'),\n",
       " ('Sullen', 'B-PER\\n'),\n",
       " ('En', 'O\\n'),\n",
       " ('alsoo', 'O\\n'),\n",
       " ('op', 'O\\n'),\n",
       " ('nieuws', 'O\\n'),\n",
       " ('disponeerende', 'O\\n'),\n",
       " ('verklaren', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Testateuren', 'B-MISC\\n'),\n",
       " ('', ''),\n",
       " ('elkanderen', 'O\\n'),\n",
       " ('over', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('wedr', 'O\\n'),\n",
       " ('dat', 'O\\n'),\n",
       " ('is', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('eerstster', 'O\\n'),\n",
       " ('vende', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Langstleevende', 'B-MISC\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('Stellen', 'B-LOC\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('', ''),\n",
       " ('institueeren', 'O\\n'),\n",
       " ('tot', 'O\\n'),\n",
       " ('Sijn', 'B-PER\\n'),\n",
       " ('offte', 'O\\n'),\n",
       " ('hare', 'O\\n'),\n",
       " ('eenige', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('imierseele', 'O\\n'),\n",
       " ('Erffgenaam', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('Erffgename', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('dat', 'O\\n'),\n",
       " ('', ''),\n",
       " ('in', 'O\\n'),\n",
       " ('alle', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('goederen', 'O\\n'),\n",
       " ('roerende', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('orroe�', 'O\\n'),\n",
       " ('rende', 'O\\n'),\n",
       " ('Actien', 'B-ORG\\n'),\n",
       " ('Coediten', 'I-ORG\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('geregtigheden', 'O\\n'),\n",
       " ('geene', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('dien', 'O\\n'),\n",
       " ('', ''),\n",
       " ('uijtgesondert', 'O\\n'),\n",
       " ('door', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Eerffstervende', 'B-MISC\\n'),\n",
       " ('met', 'O\\n'),\n",
       " ('er', 'O\\n'),\n",
       " ('dood', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('ontruijmen', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('na', 'O\\n'),\n",
       " ('telaten;', 'O\\n'),\n",
       " ('omme', 'O\\n'),\n",
       " ('', ''),\n",
       " ('alle', 'O\\n'),\n",
       " ('deselve', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('aanvaerde', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('daer', 'O\\n'),\n",
       " ('mede', 'O\\n'),\n",
       " ('doen', 'O\\n'),\n",
       " ('handelen', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('disponeeren', 'O\\n'),\n",
       " ('naer', 'O\\n'),\n",
       " ('des', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Langstlevendes', 'O\\n'),\n",
       " ('gelieffte', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('welgevallen', 'O\\n'),\n",
       " ('Sonder', 'B-MISC\\n'),\n",
       " ('tegen�', 'O\\n'),\n",
       " ('seggen', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('imand', 'B-PER\\n'),\n",
       " ('dog', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('gevalle', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Testatrice', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('eerst', 'O\\n'),\n",
       " ('Stervende', 'B-MISC\\n'),\n",
       " ('quam', 'I-MISC\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('Sijn', 'B-LOC\\n'),\n",
       " ('souder', 'O\\n'),\n",
       " ('kind', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('Kinderen', 'O\\n'),\n",
       " ('natelaten,', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('op', 'O\\n'),\n",
       " ('haer', 'O\\n'),\n",
       " ('', ''),\n",
       " ('overlijden', 'O\\n'),\n",
       " ('haer', 'O\\n'),\n",
       " ('Moeder', 'O\\n'),\n",
       " ('Trijntje', 'B-PER\\n'),\n",
       " ('harsinklaastwede:', 'O\\n'),\n",
       " ('Simon', 'B-PER\\n'),\n",
       " ('van', 'I-PER\\n'),\n",
       " ('Zuijderwijk', 'I-PER\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('dan', 'O\\n'),\n",
       " ('nog', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Leeven', 'O\\n'),\n",
       " ('was,', 'O\\n'),\n",
       " ('sal', 'O\\n'),\n",
       " ('den', 'O\\n'),\n",
       " ('Testateur', 'B-PER\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('dangehouden', 'O\\n'),\n",
       " ('Sijn', 'B-PER\\n'),\n",
       " ('om', 'O\\n'),\n",
       " ('aan', 'O\\n'),\n",
       " ('des', 'O\\n'),\n",
       " ('Testatrices', 'B-MISC\\n'),\n",
       " ('voornoemde', 'O\\n'),\n",
       " ('Moede', 'B-PER\\n'),\n",
       " ('', ''),\n",
       " ('uijt', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('keeren', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Legitime', 'B-MISC\\n'),\n",
       " ('portie', 'O\\n'),\n",
       " ('deselve', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('goederen', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Testatrice', 'B-ORG\\n'),\n",
       " ('na', 'O\\n'),\n",
       " ('regten', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Competeerende', 'O\\n'),\n",
       " ('verklaren', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Testatrice', 'B-MISC\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('dan', 'O\\n'),\n",
       " ('deselve', 'O\\n'),\n",
       " ('hare', 'O\\n'),\n",
       " ('Moeder', 'B-PER\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('Soo', 'B-LOC\\n'),\n",
       " ('verre', 'O\\n'),\n",
       " ('tot', 'O\\n'),\n",
       " ('haer', 'O\\n'),\n",
       " ('mede', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Erffgename', 'O\\n'),\n",
       " ('ten', 'O\\n'),\n",
       " ('institueeren', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('verder', 'O\\n'),\n",
       " ('nog', 'O\\n'),\n",
       " ('anders', 'O\\n'),\n",
       " ('niet', 'O\\n'),\n",
       " ('Dit', 'O\\n'),\n",
       " ('alles', 'O\\n'),\n",
       " ('indien', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('eerst', 'O\\n'),\n",
       " ('stervende', 'O\\n'),\n",
       " ('', ''),\n",
       " ('der', 'B-PER\\n'),\n",
       " ('Testateure', 'B-ORG\\n'),\n",
       " ('uijt', 'O\\n'),\n",
       " ('desen', 'O\\n'),\n",
       " ('huwelijke', 'O\\n'),\n",
       " ('verwekt', 'O\\n'),\n",
       " ('sonder', 'O\\n'),\n",
       " ('kind', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('kinderen', 'O\\n'),\n",
       " ('^', 'O\\n'),\n",
       " ('natelaten', 'O\\n'),\n",
       " ('komt', 'O\\n'),\n",
       " ('', ''),\n",
       " ('te', 'O\\n'),\n",
       " ('overlijd', 'O\\n'),\n",
       " ('maer', 'O\\n'),\n",
       " ('kind', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('kinderen', 'O\\n'),\n",
       " ('nablijvende', 'O\\n'),\n",
       " ('Soo', 'B-PER\\n'),\n",
       " ('verklaren', 'O\\n'),\n",
       " ('^', 'O\\n'),\n",
       " ('wel', 'O\\n'),\n",
       " ('mede', 'O\\n'),\n",
       " ('Testateuren', 'B-MISC\\n'),\n",
       " ('elkanderen', 'O\\n'),\n",
       " ('', ''),\n",
       " ('^', 'O\\n'),\n",
       " ('over', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('weder', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('Stellen', 'B-LOC\\n'),\n",
       " ('tot', 'O\\n'),\n",
       " ('Erffg', 'O\\n'),\n",
       " ('naem', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('Erffgename', 'O\\n'),\n",
       " ('invoegen', 'O\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('hier', 'O\\n'),\n",
       " ('voren', 'O\\n'),\n",
       " ('is', 'O\\n'),\n",
       " ('gemeld', 'O\\n'),\n",
       " ('', ''),\n",
       " ('dog', 'O\\n'),\n",
       " ('met', 'O\\n'),\n",
       " ('dien', 'O\\n'),\n",
       " ('verstande', 'O\\n'),\n",
       " ('nogtans', 'O\\n'),\n",
       " ('dat', 'O\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('dan', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Langt', 'B-MISC\\n'),\n",
       " ('leevende', 'O\\n'),\n",
       " ('gehouden', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('verpligt', 'O\\n'),\n",
       " ('Sal', 'B-PER\\n'),\n",
       " ('Sijn', 'I-PER\\n'),\n",
       " ('', ''),\n",
       " ('deselve', 'O\\n'),\n",
       " ('hunne', 'O\\n'),\n",
       " ('kind', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('kinderen', 'O\\n'),\n",
       " ('eerlijk', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('naer', 'O\\n'),\n",
       " ('Staats', 'B-MISC\\n'),\n",
       " ('gelegend', 'O\\n'),\n",
       " ('heijd', 'O\\n'),\n",
       " ('op', 'O\\n'),\n",
       " ('tevoeden', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('', ''),\n",
       " ('groottemaken', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('komende', 'O\\n'),\n",
       " ('ten', 'O\\n'),\n",
       " ('mondigi', 'O\\n'),\n",
       " ('dage', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('huwelijken', 'O\\n'),\n",
       " ('state', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('ook', 'O\\n'),\n",
       " ('ingevalle', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Lang', 'B-MISC\\n'),\n",
       " ('', ''),\n",
       " ('leevende', 'O\\n'),\n",
       " ('weder', 'O\\n'),\n",
       " ('quam', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('hertrouwen', 'O\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('dan', 'O\\n'),\n",
       " ('deselve', 'O\\n'),\n",
       " ('kinderen', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('doteeren', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('uijttesetten', 'O\\n'),\n",
       " ('', ''),\n",
       " ('off', 'O\\n'),\n",
       " ('aan', 'O\\n'),\n",
       " ('hun', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('bewijse', 'O\\n'),\n",
       " ('soo', 'O\\n'),\n",
       " ('veel', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('soo', 'O\\n'),\n",
       " ('wijnig', 'O\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Langstleevende', 'B-MISC\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('dan', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('goeden', 'O\\n'),\n",
       " ('gemoede', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('', ''),\n",
       " ('naer', 'O\\n'),\n",
       " ('gelegindheijdt', 'O\\n'),\n",
       " ('des', 'O\\n'),\n",
       " ('boedel', 'O\\n'),\n",
       " ('sal', 'O\\n'),\n",
       " ('oordeelen', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('behoren;', 'O\\n'),\n",
       " ('welke', 'O\\n'),\n",
       " ('opvoeding', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('groot', 'O\\n'),\n",
       " ('making', 'O\\n'),\n",
       " ('', ''),\n",
       " ('mitsgaders', 'O\\n'),\n",
       " ('uijtset', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('bewijs', 'O\\n'),\n",
       " ('hij', 'O\\n'),\n",
       " ('Testateuren', 'B-MISC\\n'),\n",
       " ('verklaren', 'O\\n'),\n",
       " ('hunne', 'O\\n'),\n",
       " ('voorne:', 'O\\n'),\n",
       " ('Kind', 'B-MISC\\n'),\n",
       " ('off', 'I-MISC\\n'),\n",
       " ('Kinderen', 'O\\n'),\n",
       " ('', ''),\n",
       " ('toetevoege', 'O\\n'),\n",
       " ('voor', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('plaatse', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('hunne', 'O\\n'),\n",
       " ('Legitime', 'B-MISC\\n'),\n",
       " ('portie', 'O\\n'),\n",
       " ('hun', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('goederen', 'O\\n'),\n",
       " ('vande', 'O\\n'),\n",
       " ('eerst', 'O\\n'),\n",
       " ('Stervende', 'B-PER\\n'),\n",
       " ('', ''),\n",
       " ('Competeerende', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('Soo', 'B-LOC\\n'),\n",
       " ('verre', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('selve', 'O\\n'),\n",
       " ('hunne', 'O\\n'),\n",
       " ('kind', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('kinderen', 'O\\n'),\n",
       " ('tot', 'O\\n'),\n",
       " ('hun', 'O\\n'),\n",
       " ('mede', 'O\\n'),\n",
       " ('erffgenamen', 'O\\n'),\n",
       " ('', ''),\n",
       " ('institueerende', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('verder', 'O\\n'),\n",
       " ('nog', 'O\\n'),\n",
       " ('ander', 'O\\n'),\n",
       " ('nmet', 'O\\n'),\n",
       " ('Stellende', 'B-MISC\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('Committeerende', 'O\\n'),\n",
       " ('ook', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Testateuren', 'B-MISC\\n'),\n",
       " ('', ''),\n",
       " ('elkanderen', 'O\\n'),\n",
       " ('over', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('weder', 'O\\n'),\n",
       " ('tot', 'O\\n'),\n",
       " ('absolute', 'O\\n'),\n",
       " ('Voogd', 'B-MISC\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('Voogdes,', 'B-PER\\n'),\n",
       " ('arije', 'O\\n'),\n",
       " ('oven', 'O\\n'),\n",
       " ('hunne', 'O\\n'),\n",
       " ('natelatene', 'O\\n'),\n",
       " ('^', 'O\\n'),\n",
       " ('kind', 'O\\n'),\n",
       " ('', ''),\n",
       " ('off', 'O\\n'),\n",
       " ('kinderen', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('tot', 'O\\n'),\n",
       " ('admi�', 'O\\n'),\n",
       " ('nistrateur', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('administratrice', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('derselver', 'O\\n'),\n",
       " ('goederen', 'O\\n'),\n",
       " ('gevende', 'O\\n'),\n",
       " ('elkanderen', 'O\\n'),\n",
       " ('over', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('weder', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Sodanige', 'O\\n'),\n",
       " ('magt', 'O\\n'),\n",
       " ('Lasten', 'O\\n'),\n",
       " ('authoriteijt', 'O\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('aan', 'O\\n'),\n",
       " ('Voogden', 'B-PER\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('administrateurs', 'B-PER\\n'),\n",
       " ('na', 'O\\n'),\n",
       " ('regten', 'O\\n'),\n",
       " ('gegeeven', 'O\\n'),\n",
       " ('kan', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('', ''),\n",
       " ('mog', 'O\\n'),\n",
       " ('werden', 'O\\n'),\n",
       " ('met', 'O\\n'),\n",
       " ('magt', 'O\\n'),\n",
       " ('om', 'O\\n'),\n",
       " ('imand', 'O\\n'),\n",
       " ('tot', 'O\\n'),\n",
       " ('Sig', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('assumeeren', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('desselffs', 'O\\n'),\n",
       " ('plaatse', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('Surrogeeren', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('voorts', 'O\\n'),\n",
       " ('met', 'O\\n'),\n",
       " ('', ''),\n",
       " ('eenbiedige', 'O\\n'),\n",
       " ('uijtsluijtinge', 'O\\n'),\n",
       " ('Soo', 'B-PER\\n'),\n",
       " ('ter', 'I-PER\\n'),\n",
       " ('Laaster', 'I-PER\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('eerster', 'O\\n'),\n",
       " ('Dood', 'B-MISC\\n'),\n",
       " ('van', 'I-MISC\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('heeren', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Weesmeesteren', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('geregten', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('dese', 'O\\n'),\n",
       " ('Stad', 'I-LOC\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('alle', 'O\\n'),\n",
       " ('andre', 'O\\n'),\n",
       " ('Steden', 'B-LOC\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('plaatsen', 'O\\n'),\n",
       " ('daer', 'O\\n'),\n",
       " ('hun', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Sterffhuijs', 'O\\n'),\n",
       " ('Soude', 'O\\n'),\n",
       " ('mogen', 'O\\n'),\n",
       " ('^', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('vallen', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('eenige', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('hunne', 'O\\n'),\n",
       " ('goederen', 'O\\n'),\n",
       " ('mog', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('geleegen', 'O\\n'),\n",
       " ('Sijn,', 'B-PER\\n'),\n",
       " ('', ''),\n",
       " ('deselve', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('alle', 'O\\n'),\n",
       " ('diuictie', 'O\\n'),\n",
       " ('excuseerende', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('voor', 'O\\n'),\n",
       " ('hun', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('neemene', 'O\\n'),\n",
       " ('moeijten', 'O\\n'),\n",
       " ('bedankende', 'O\\n'),\n",
       " ('bij', 'O\\n'),\n",
       " ('', ''),\n",
       " ('desen', 'O\\n'),\n",
       " ('Alle', 'O\\n'),\n",
       " (\"'t\", 'O\\n'),\n",
       " ('geene', 'O\\n'),\n",
       " ('voorsz:', 'O\\n'),\n",
       " ('staet', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Testateuren', 'B-ORG\\n'),\n",
       " ('duijdelijk', 'O\\n'),\n",
       " ('voorgelegen', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Sijnde', 'B-PER\\n'),\n",
       " ('verklaerden', 'O\\n'),\n",
       " ('ij', 'O\\n'),\n",
       " (\"'t\", 'O\\n'),\n",
       " ('Selve', 'B-PER\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('wesen', 'O\\n'),\n",
       " ('hun', 'O\\n'),\n",
       " ('testament', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('uijtterste', 'O\\n'),\n",
       " ('wille;', 'O\\n'),\n",
       " ('', ''),\n",
       " ('willende', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('begeerende', 'O\\n'),\n",
       " ('dat', 'O\\n'),\n",
       " (\"'t\", 'O\\n'),\n",
       " ('Selve', 'B-ORG\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('sodanig', 'O\\n'),\n",
       " ('offte', 'O\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('Codicil', 'B-ORG\\n'),\n",
       " ('giffte', 'O\\n'),\n",
       " ('onder', 'O\\n'),\n",
       " ('den', 'O\\n'),\n",
       " ('', ''),\n",
       " ('leevende', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('ter', 'O\\n'),\n",
       " ('Sake', 'B-MISC\\n'),\n",
       " ('des', 'I-MISC\\n'),\n",
       " ('Doods', 'I-MISC\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('Soo', 'B-PER\\n'),\n",
       " ('als', 'O\\n'),\n",
       " (\"'t\", 'O\\n'),\n",
       " ('Selve', 'O\\n'),\n",
       " ('best', 'O\\n'),\n",
       " ('sal', 'O\\n'),\n",
       " ('konnen', 'O\\n'),\n",
       " ('bestaan', 'O\\n'),\n",
       " ('', ''),\n",
       " ('onverbreekelijk', 'O\\n'),\n",
       " ('Sal', 'B-MISC\\n'),\n",
       " ('moeten', 'O\\n'),\n",
       " ('werden', 'O\\n'),\n",
       " ('nagekomen', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('agtervolgt', 'O\\n'),\n",
       " ('Schoon', 'O\\n'),\n",
       " ('al', 'O\\n'),\n",
       " ('eenige', 'O\\n'),\n",
       " ('solemniteijten', 'O\\n'),\n",
       " ('na', 'O\\n'),\n",
       " ('', ''),\n",
       " ('regten', 'O\\n'),\n",
       " ('off', 'O\\n'),\n",
       " ('Costmen', 'B-PER\\n'),\n",
       " ('vereijst', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('desen', 'O\\n'),\n",
       " ('niet', 'O\\n'),\n",
       " ('na', 'O\\n'),\n",
       " ('behoren', 'O\\n'),\n",
       " ('mogten', 'O\\n'),\n",
       " ('Sijn', 'B-PER\\n'),\n",
       " ('geobserveerd', 'O\\n'),\n",
       " ('En', 'O\\n'),\n",
       " ('', ''),\n",
       " ('versogten', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('Testateuren', 'B-MISC\\n'),\n",
       " ('aan', 'O\\n'),\n",
       " ('mij', 'O\\n'),\n",
       " ('Notaris', 'B-PER\\n'),\n",
       " ('hier', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('maken', 'O\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('aan', 'O\\n'),\n",
       " ('hun', 'O\\n'),\n",
       " ('te', 'O\\n'),\n",
       " ('leeveren', 'O\\n'),\n",
       " ('instrument', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('', ''),\n",
       " ('testament', 'O\\n'),\n",
       " ('in', 'O\\n'),\n",
       " ('behoorlijke', 'O\\n'),\n",
       " ('forma', 'O\\n'),\n",
       " ('dat', 'O\\n'),\n",
       " ('aldus', 'O\\n'),\n",
       " ('passeerde', 'O\\n'),\n",
       " ('binnen', 'O\\n'),\n",
       " ('Amsterdam', 'B-LOC\\n'),\n",
       " ('ten', 'O\\n'),\n",
       " ('woon�', 'O\\n'),\n",
       " ('huijse', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('de', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Testateuren', 'O\\n'),\n",
       " ('voorne:', 'O\\n'),\n",
       " ('ter', 'O\\n'),\n",
       " ('presentie', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " (\"d'\", 'B-PER\\n'),\n",
       " ('E.', 'B-PER\\n'),\n",
       " ('Hendrik', 'I-PER\\n'),\n",
       " ('brouwer', 'I-PER\\n'),\n",
       " ('en', 'O\\n'),\n",
       " ('Schilip', 'B-PER\\n'),\n",
       " ('van', 'I-PER\\n'),\n",
       " ('Soest', 'I-PER\\n'),\n",
       " ('als', 'O\\n'),\n",
       " ('betuigen', 'O\\n'),\n",
       " ('', ''),\n",
       " ('Mansenun', 'B-PER\\n'),\n",
       " ('Cijmanunj', 'I-PER\\n'),\n",
       " ('Margreta', 'I-PER\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('Luijderijk', 'B-LOC\\n'),\n",
       " ('Hendenk', 'B-PER\\n'),\n",
       " ('Boouwes', 'I-PER\\n'),\n",
       " ('Jclip', 'O\\n'),\n",
       " ('van', 'O\\n'),\n",
       " ('', ''),\n",
       " ('voest', 'O\\n'),\n",
       " ('Jan', 'B-PER\\n'),\n",
       " ('herleid', 'I-PER\\n'),\n",
       " ('Notsden', 'I-PER\\n'),\n",
       " ('16e:', 'O\\n'),\n",
       " ('maert', 'O\\n'),\n",
       " ('1741', 'O\\n'),\n",
       " (\"'S\", 'O\\n'),\n",
       " ('avonts', 'O\\n'),\n",
       " ('over', 'O\\n'),\n",
       " ('7uuren', 'O\\n')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_bert = pd.read_csv('full_bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb4c6c183ea432db2914e627de480d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13063), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ner_result = full_bert.progress_apply(NER_test, args=['BERTje', 2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fp = sum([x['fp'] for x in ner_result])\n",
    "total_tp = sum([x['tp'] for x in ner_result])\n",
    "total_fn = sum([x['fn'] for x in ner_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438764, 48911, 29461, 0.1002942533449531, 0.6240876843770734)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_fp,total_tp,total_fn, total_tp / (total_tp + total_fp), total_tp / (total_tp + total_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTje Tests\n",
    "\n",
    "#### BERTje SpaCy\n",
    "TP:   \n",
    "FP:   \n",
    "FN:   \n",
    "Recall: 0.  \n",
    "Precision: 0.  \n",
    "\n",
    "#### BERTje met Levenshtein distance 2 en achternaam removal\n",
    "TP: 48911  \n",
    "FP:   438764  \n",
    "FN:  29461  \n",
    "Recall:  0.6240876843770734  \n",
    "Precision: 0.1002942533449531\n",
    "\n",
    "#### BERTje met Levenshtein distance 3 en achternaam removal\n",
    "TP: 53766  \n",
    "FP: 434134  \n",
    "FN: 26860  \n",
    "Recall: 0.6668568451864163  \n",
    "Precision: 0.1101988112318098  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robBERT-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"pdelobelle/robBERT-base\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
