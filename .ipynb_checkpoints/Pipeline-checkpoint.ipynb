{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import notebook\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import timeit\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import Session\n",
    "from tensorflow.python.saved_model import loader\n",
    "notebook.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load('../Spacy models/kfold_3', disable=['parser', 'tagger', 'textcat'])\n",
    "notary = pd.read_csv('../clean_data.csv')\n",
    "voc = pd.read_csv('vocop_clustered_dutchrank.csv')\n",
    "#voc = pd.read_csv('vocop-clustered-new.csv', sep='\t')\n",
    "#rangen = pd.read_excel('../vocop_rangen.xlsx', index_col=0)\n",
    "#voc['dutch_rank'] = [translate_rank(x, rangen) for x in notebook.tqdm(voc['rank'].tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER(text, nlp):\n",
    "    ''' Takes in a string and uses the specified NLP model to tag entities within the string.\n",
    "        Returns all entities tagged as PERSON within the string.\n",
    "    '''\n",
    "    # Tag text\n",
    "    doc = nlp(text)\n",
    "    holder = []\n",
    "    \n",
    "    # Append all entities tagged as PERSON\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON' and ' ' in ent.text:\n",
    "            holder.append(ent.text)\n",
    "    return holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(values):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    def _float_feature(value):\n",
    "        \"\"\"Returns an float_list from a int/float.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "    \n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    # data type.\n",
    "    feature = {}\n",
    "    for x in enumerate(values):\n",
    "        feature[str(x[0] + 1)] = _float_feature(values[x[0]])\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def ranking(matches, directory):\n",
    "    tags=[\"serve\"]\n",
    "    signature_def_key = \"predict\"\n",
    "    saved_model_dir = directory\n",
    "    holder = []\n",
    "    with Session() as sess:\n",
    "        loader.load(sess, tags, saved_model_dir)\n",
    "        serialized_examples = []\n",
    "        vocop_ids = [x for x in matches]\n",
    "        for x in matches:\n",
    "            names = matches[x][0]\n",
    "            days = matches[x][2]\n",
    "            locations = matches[x][3]\n",
    "            ranks = matches[x][4]\n",
    "            ships = matches[x][5]\n",
    "            name_count = matches[x][1]\n",
    "            serialized_example = serialize_example([names, name_count, days, locations, \n",
    "                                                   ranks, ships] )\n",
    "            serialized_examples.append(serialized_example)\n",
    "            inputs_feed_dict = {'input_example_tensor:0': serialized_examples}\n",
    "            outputs = sess.run('groupwise_dnn_v2/accumulate_scores/div_no_nan:0', feed_dict=inputs_feed_dict)\n",
    "            output = [(outputs[x], vocop_ids[x]) for x in range(len(outputs))]\n",
    "            holder = output\n",
    "    return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_neighbour(start, end, true, prev, distance):\n",
    "    if true == []:\n",
    "        return (start, end), prev.i\n",
    "    if prev.i == len(prev.doc) - 1:\n",
    "        return (start, end), prev.i\n",
    "    if fuzz.ratio(true[0].lower(), prev.nbor().text.lower()) >= distance:\n",
    "        return match_neighbour(start, prev.nbor().idx + len(prev.nbor()), true[1:], prev.nbor(), distance)\n",
    "    else:\n",
    "        return (start, end), prev.i\n",
    "\n",
    "def match_finder(row, match, distance):\n",
    "\n",
    "    true = match\n",
    "    doc = nlp2(row)\n",
    "    locs = []\n",
    "    prev = 0\n",
    "    for token in doc:\n",
    "            for x in true:\n",
    "                if token.i > prev and type(x) == str:\n",
    "                    split = x.split(' ')\n",
    "                    if fuzz.ratio(split[0].lower(), token.text.lower()) >= distance:\n",
    "                        result, prev = match_neighbour(token.idx, token.idx + len(token), split[1:], token, distance)\n",
    "                        if result not in locs:\n",
    "                            locs.append(result)\n",
    "    entities = [row[x[0]:x[1]] for x in locs]\n",
    "    return entities\n",
    "\n",
    "def find_matches(name, knowledgebase, target_column, distance=90):\n",
    "    ''' Takes in a string containing the name of a person and returns all possible matches from the\n",
    "        knowledgebase based on fuzzy string matching.\n",
    "    '''\n",
    "    matches = knowledgebase[(knowledgebase[target_column].str.lower().astype(str).apply(fuzz.ratio, args=[name.lower()]) >= distance)]\n",
    "    return matches\n",
    "\n",
    "def convert_matches(entity, matches, name_dict):\n",
    "    \n",
    "    holder = {}\n",
    "    notary_date = datetime.strptime(entity.datering, '%Y-%m-%d')\n",
    "    \n",
    "    # Try to convert the dates for each entry into datetime format\n",
    "    for x in range(matches.shape[0]):\n",
    "        try:\n",
    "            date1 = datetime.strptime(matches['date_begin_service_complete'].iloc[x], '%Y-%m-%d')\n",
    "        except:\n",
    "            date1 = datetime(year=1, month=1, day =1 )\n",
    "        try:\n",
    "            date2 = datetime.strptime(matches['date_end_service_complete'].iloc[x], '%Y-%m-%d')\n",
    "        except:\n",
    "            date2 = datetime(year=1, month=1, day =1 )\n",
    "            \n",
    "        # Keep only matches that are within distance days from notary_date\n",
    "        if abs((notary_date - date1).days) < 90 or abs((notary_date - date2).days) < 90:\n",
    "            name_ratio = fuzz.ratio(entity.naam, matches['fullNameOriginal'].iloc[x])\n",
    "            name_count = name_dict[matches['fullNameNormalized'].iloc[x]]\n",
    "            vocid=matches['VOCOP_id'].iloc[x]\n",
    "            day_dif = min([abs((notary_date - date1).days), abs((notary_date - date2).days)])\n",
    "            location = len(match_finder(entity.text, [matches['placeOfOrigin'].iloc[x]], 80))\n",
    "            rank = len(match_finder(entity.text, [matches['dutch_rank'].iloc[x]], 80))\n",
    "            numships = len(match_finder(entity.text, [matches['shipOutward'].iloc[x], matches['shipReturn'].iloc[x]], 80))\n",
    "            holder[vocid] = [name_ratio, name_count, day_dif, location, rank, numships]\n",
    "\n",
    "    return holder\n",
    "\n",
    "def NEL(entity, knowledgebase, model_dir):\n",
    "    ''' Takes in a string containing an entity and returns either a match within the specified pandas \n",
    "        knowledgebase or None if no match is present.\n",
    "    ''' \n",
    "    # Find and narrow down matches\n",
    "    possible_matches = find_matches(entity.naam, knowledgebase, 'fullNameOriginal')\n",
    "    name_dict = knowledgebase.fullNameNormalized.value_counts()\n",
    "    converted_matches = convert_matches(entity, possible_matches, name_dict)\n",
    "    if converted_matches == {}:\n",
    "        return None\n",
    "    ranked_matches = ranking(converted_matches, model_dir)\n",
    "    return max(ranked_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pipeline(row, knowledgebase, NER_model, NEL_model):\n",
    "    holder = []\n",
    "    entities = NER(row.text, NER_model)\n",
    "    for x in entities:\n",
    "        row['naam'] = x\n",
    "        holder.append(NEL(row, knowledgebase, NEL_model))\n",
    "    return holder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________\n",
      "Pieter Klijn\n",
      "{764045: [96, 46, 18, 0, 0, 0], 784359: [100, 46, 7, 0, 0, 0], 802002: [100, 46, 55, 3, 0, 2], 814032: [100, 46, 38, 3, 0, 0]}\n",
      "INFO:tensorflow:Restoring parameters from ranking_crossvalidation/kfold5/export/1591180581\\variables\\variables\n",
      "_______________________________________\n",
      "Jan Backer\n",
      "{1150409: [90, 91, 88, 0, 0, 0], 1154910: [90, 91, 47, 3, 0, 0]}\n",
      "INFO:tensorflow:Restoring parameters from ranking_crossvalidation/kfold5/export/1591180581\\variables\\variables\n",
      "_______________________________________\n",
      "Jan Backer\n",
      "{1150409: [90, 91, 88, 0, 0, 0], 1154910: [90, 91, 47, 3, 0, 0]}\n",
      "INFO:tensorflow:Restoring parameters from ranking_crossvalidation/kfold5/export/1591180581\\variables\\variables\n",
      "Seconds for single query: 22.93101619999925\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "x = Pipeline(notary.iloc[85], voc, nlp2, 'ranking_crossvalidation/kfold5/export/1591180581')\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('Seconds for single query: ' + str(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([2.1179523], dtype=float32), 802002),\n",
       " (array([-17.653296], dtype=float32), 1150409),\n",
       " (array([-17.653296], dtype=float32), 1150409)]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802002"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.iloc[284049]['VOCOP_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>rubriek</th>\n",
       "      <th>notaris</th>\n",
       "      <th>inventarisNr</th>\n",
       "      <th>akteNr</th>\n",
       "      <th>akteType</th>\n",
       "      <th>datering</th>\n",
       "      <th>taal</th>\n",
       "      <th>beschrijving</th>\n",
       "      <th>namen</th>\n",
       "      <th>urls</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>ee21c8bc-bfea-25ee-c6c1-ddf74644012b</td>\n",
       "      <td>358</td>\n",
       "      <td>JAN VERLEIJ</td>\n",
       "      <td>11890</td>\n",
       "      <td>12427</td>\n",
       "      <td>Obligatie</td>\n",
       "      <td>1744-09-12</td>\n",
       "      <td>nederlands</td>\n",
       "      <td>\\nschip Diemen\\n</td>\n",
       "      <td>[{'voornaam': 'Pieter', 'tussenvoegsel': None,...</td>\n",
       "      <td>['KLAB05439000262.JPG', 'KLAB05439000263.JPG',...</td>\n",
       "      <td>Ragd werlke Getijnke Gepasseert Den 12 feptemb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    uuid  rubriek      notaris  inventarisNr  \\\n",
       "85  ee21c8bc-bfea-25ee-c6c1-ddf74644012b      358  JAN VERLEIJ         11890   \n",
       "\n",
       "    akteNr   akteType    datering        taal      beschrijving  \\\n",
       "85   12427  Obligatie  1744-09-12  nederlands  \\nschip Diemen\\n   \n",
       "\n",
       "                                                namen  \\\n",
       "85  [{'voornaam': 'Pieter', 'tussenvoegsel': None,...   \n",
       "\n",
       "                                                 urls  \\\n",
       "85  ['KLAB05439000262.JPG', 'KLAB05439000263.JPG',...   \n",
       "\n",
       "                                                 text  \n",
       "85  Ragd werlke Getijnke Gepasseert Den 12 feptemb...  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notary[notary.uuid == 'ee21c8bc-bfea-25ee-c6c1-ddf74644012b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('slot van kapelle', 'slot capelle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
