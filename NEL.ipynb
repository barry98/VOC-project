{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import notebook\n",
    "import ast\n",
    "\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "from tensorflow.compat.v1 import Session\n",
    "from tensorflow.python.saved_model import loader\n",
    "from sklearn.model_selection import KFold\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook.tqdm.pandas()\n",
    "clean = pd.read_csv('../clean_data.csv')\n",
    "voc = pd.read_csv('vocop-clustered-new.csv', sep='\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid = []\n",
    "name = []\n",
    "for y, z in notebook.tqdm(clean.iterrows()):\n",
    "    for x in ast.literal_eval(z.namen):\n",
    "        if x['tussenvoegsel'] != None:\n",
    "            name.append(x['voornaam'] + \" \" + x['tussenvoegsel'] + \" \" + x['achternaam'])\n",
    "            uuid.append(z.uuid)\n",
    "        elif x['voornaam'] and x['achternaam'] != None:\n",
    "            name.append(x['voornaam'] + \" \" + x['achternaam'])\n",
    "            uuid.append(z.uuid)\n",
    "name_list = pd.DataFrame(data={'uuid':uuid, 'name':name}, columns=['uuid', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df = clean.merge(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_search(name, distance):\n",
    "    names = np.where((voc.fullNameNormalized.apply(fuzz.ratio, args=[name]) >= 90) | \n",
    "                     (voc.fullNameOriginal.dropna().apply(fuzz.ratio, args=[name]) >= 90))\n",
    "    final = (name, names)\n",
    "    return final\n",
    "\n",
    "def find_matches(names, distance):\n",
    "    name_list = {}\n",
    "    final = []\n",
    "    for x in notebook.tqdm(names):\n",
    "        if x in name_list:\n",
    "            final.append((x, name_list[x]))\n",
    "        else:\n",
    "            result = fuzzy_search(x, distance)\n",
    "            name_list[x] = result\n",
    "            final.append((x, result))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking\n",
    "\n",
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preranking.csv')\n",
    "df['is_nil'] = 0\n",
    "for x in df.notary_id.unique():\n",
    "    nil = {}\n",
    "    values = df[df.notary_id==x]\n",
    "    for y in df:\n",
    "        nil[y] = values[y].max()\n",
    "    nil['name_count'] = values.name_count.min()\n",
    "    nil['day_dif'] = values.day_dif.min()\n",
    "    nil['voc_id'] = 'NIL'\n",
    "    nil['is_nil'] = 1\n",
    "    if values.match.mean() > 0:\n",
    "        nil['match'] = 0\n",
    "    else:\n",
    "        nil['match'] = 1\n",
    "    nil_df = pd.DataFrame(data=nil, index=[-1])\n",
    "    df = pd.concat([df,nil_df])\n",
    "df = df.reset_index(drop = True)\n",
    "indexes = [x for x in np.random.choice(df.notary_id.unique(), int(len(df.notary_id.unique()) *0.8), replace=False)]\n",
    "train = df[df.notary_id.isin(indexes)]\n",
    "testval = df[df.notary_id.isin(indexes) == False]\n",
    "\n",
    "test_indexes = [x for x in np.random.choice(testval.notary_id.unique(), len(testval.notary_id.unique()) // 2, replace=False)]\n",
    "test = testval[testval.notary_id.isin(test_indexes)]\n",
    "val = testval[testval.notary_id.isin(test_indexes) == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_json('train7.json')\n",
    "# test.to_json('test7.json')\n",
    "# val.to_json('val7.json')\n",
    "\n",
    "# train = pd.read_json('trainltr.json')\n",
    "train = pd.read_json('train7.json')\n",
    "# test = pd.read_json('testltr.json')\n",
    "test = pd.read_json('test7.json')\n",
    "# val = pd.read_json('valltr.json')\n",
    "val = pd.read_json('val7.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train1 = train\n",
    "# test1 = test\n",
    "# val1 = val\n",
    "# train2 = train\n",
    "# test2 = test\n",
    "# val2 = val\n",
    "#train3 = train\n",
    "#test3 = test\n",
    "#val3 = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subcolumns = ['name_ratio', 'name_count', 'day_dif', 'location', 'rank', 'numships', 'is_nil'] \n",
    "# subcolumns = ['name_ratio', 'name_count', 'day_dif', 'location', 'rank', 'numships'] \n",
    "subcolumns = ['name_ratio', 'name_count', 'day_dif', 'location', 'rank', 'numships', 'keywords', 'is_nil'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('train.txt', 'w')\n",
    "for x in train.itertuples():\n",
    "    line = str(x.match) + ' qid:' + str(x.notary_id)\n",
    "    for y in enumerate(subcolumns):\n",
    "        if getattr(x, y[1]) != 0:\n",
    "            line = line + ' ' + str(y[0] + 1) + ':' + str(getattr(x, y[1]))\n",
    "    if x.Index + 1 == df.shape[0]:\n",
    "        file.writelines(line)\n",
    "    else:\n",
    "        file.writelines(line + '\\n')\n",
    "file.close()\n",
    "\n",
    "file = open('test.txt', 'w')\n",
    "for x in test.itertuples():\n",
    "    line = str(x.match) + ' qid:' + str(x.notary_id)\n",
    "    for y in enumerate(subcolumns):\n",
    "        if getattr(x, y[1]) != 0:\n",
    "            line = line + ' ' + str(y[0] + 1) + ':' + str(getattr(x, y[1]))\n",
    "    if x.Index + 1 == df.shape[0]:\n",
    "        file.writelines(line)\n",
    "    else:\n",
    "        file.writelines(line + '\\n')\n",
    "file.close()\n",
    "\n",
    "file = open('vali.txt', 'w')\n",
    "for x in val.itertuples():\n",
    "    line = str(x.match) + ' qid:' + str(x.notary_id)\n",
    "    for y in enumerate(subcolumns):\n",
    "        if getattr(x, y[1]) != 0:\n",
    "            line = line + ' ' + str(y[0] + 1) + ':' + str(getattr(x, y[1]))\n",
    "    if x.Index + 1 == df.shape[0]:\n",
    "        file.writelines(line)\n",
    "    else:\n",
    "        file.writelines(line + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !saved_model_cli show \\\n",
    "#     --dir six_features/export/1590418714 \\\n",
    "#     --tag_set serve \\\n",
    "#     --signature_def predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(values, subcolumn):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    def _float_feature(value):\n",
    "        \"\"\"Returns an float_list from a int/float.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "    \n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    # data type.\n",
    "    feature = {}\n",
    "    for x in enumerate(subcolumn):\n",
    "        feature[str(x[0] + 1)] = _float_feature(values[x[1]])\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def predict(rows, id_list, directory):\n",
    "    tags=[\"serve\"]\n",
    "    signature_def_key = \"predict\"\n",
    "    saved_model_dir = directory\n",
    "    holder = {}\n",
    "    with Session() as sess:\n",
    "        loader.load(sess, tags, saved_model_dir)\n",
    "        for z in id_list:\n",
    "            values = rows[rows.notary_id == z]\n",
    "            nil = {}\n",
    "            for x in values:\n",
    "                if x != 'voc_id':\n",
    "                    nil[x] = values[x].max()\n",
    "            nil['name_count'] = values.name_count.min()\n",
    "            nil['day_dif'] = values.day_dif.min()\n",
    "            nil['voc_id'] = 'NIL'\n",
    "            nil['is_nil'] = 1\n",
    "            nil_df = pd.DataFrame(data=nil, index=[-1])\n",
    "            values = pd.concat([values,nil_df])\n",
    "            serialized_examples = []\n",
    "            for i in range(len(values)):\n",
    "                serialized_example = serialize_example(values.iloc[i], subcolumns)\n",
    "                serialized_examples.append(serialized_example)\n",
    "            inputs_feed_dict = {'input_example_tensor:0': serialized_examples}\n",
    "            outputs = sess.run('groupwise_dnn_v2/accumulate_scores/div_no_nan:0', feed_dict=inputs_feed_dict)\n",
    "            output = [(outputs[y][0], values.iloc[y].voc_id) for y in range(len(outputs))]\n",
    "            #print(output)\n",
    "            holder[z] = output\n",
    "    return holder\n",
    "\n",
    "def test_ranker(df, directory):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn1 = 0\n",
    "    fn2 = 0\n",
    "    tn = 0\n",
    "    id_list = df.notary_id.unique()\n",
    "    \n",
    "    ranking = predict(df, id_list, directory)\n",
    "    for x in id_list:\n",
    "        try:\n",
    "            predicted_match = max(ranking[x])\n",
    "        except:\n",
    "            for y in ranking[x]:\n",
    "                #print(y[1])\n",
    "                if y[1] == \"NIL\":\n",
    "                    predicted_match=y\n",
    "\n",
    "        #predicted_match[1] == 'NIL' \n",
    "        if predicted_match[1] == 'NIL':\n",
    "            if df[df.notary_id == x].match.mean() > 0:\n",
    "                fn1 += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            target = df[(df.voc_id == predicted_match[1]) & (df.notary_id == x)]\n",
    "            if target.match.iloc[0] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "                if df[df.notary_id == x].match.mean() > 0:\n",
    "                    fn2 += 1\n",
    "                    \n",
    "    recall = tp / (tp + (fn1 + fn2))\n",
    "    if tp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "    print('Recall: ' + str(recall))\n",
    "    print('Precision: ' + str(precision))\n",
    "    if precision == 0:\n",
    "        print('F1: 0')\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2*((precision*recall) / (precision + recall))\n",
    "        print('F1: ' + str(f1))\n",
    "    return {'true_positives':tp, \n",
    "            'false_positives':fp, \n",
    "            'false_negatives_threshold':fn1, \n",
    "            'false_negatives_ranker':fn2, \n",
    "            'true_negatives':tn, \n",
    "            'recall':recall, \n",
    "            'precision':precision,\n",
    "            'f1':f1}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test[(test['voc_id'] != 'NIL') & (test.match == 1)])/ len(test))\n",
    "print(test_ranker(test[test['voc_id'] != 'NIL'], 'LTR_models/nil_test2/export/1591714572'))\n",
    "print('______________________________________________________________________________________')\n",
    "# print(len(test1[(test1['voc_id'] != 'NIL') & (test1.match == 1)]) / len(test1))\n",
    "# print(test_ranker(test1[test1['voc_id'] != 'NIL'], 'LTR_models/nil_test/export/1591713231'))\n",
    "# print('______________________________________________________________________________________')\n",
    "# print(len(test[(test['voc_id'] != 'NIL') & (test.match == 1)])/ len(test))\n",
    "# print(test_ranker(test[test['voc_id'] != 'NIL'], 'LTR_models/nil_test/export/1591707535'))\n",
    "# print('______________________________________________________________________________________')\n",
    "\n",
    "# print(len(test3[(test3['voc_id'] != 'NIL') & (test3.match == 1)])/ len(test3))\n",
    "# print(test_ranker(test3[test3['voc_id'] != 'NIL'], 'LTR_models/nil_test4/export/1591711791'))\n",
    "# print('______________________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranker_validation(df, directory, threshold):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn1 = 0\n",
    "    fn2 = 0\n",
    "    tn = 0\n",
    "    id_list = df.notary_id.unique()\n",
    "    \n",
    "    ranking = predict(df, id_list, directory)\n",
    "    for x in id_list:\n",
    "        predicted_match = max(ranking[x])\n",
    "\n",
    "        if predicted_match[0] <= threshold:\n",
    "            if df[df.notary_id == x].match.mean() > 0:\n",
    "                fn1 += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            target = df[(df.voc_id == predicted_match[1]) & (df.notary_id == x)]\n",
    "            if target.match.iloc[0] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "                if df[df.notary_id == x].match.mean() > 0:\n",
    "                    fn2 += 1\n",
    "                    \n",
    "    recall = tp / (tp + (fn1 + fn2))\n",
    "    if tp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "    print('Recall: ' + str(recall))\n",
    "    print('Precision: ' + str(precision))\n",
    "    if precision == 0:\n",
    "        print('F1: 0')\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2*((precision*recall) / (precision + recall))\n",
    "        print('F1: ' + str(f1))\n",
    "    return {'true_positives':tp, \n",
    "            'false_positives':fp, \n",
    "            'false_negatives_threshold':fn1, \n",
    "            'false_negatives_ranker':fn2, \n",
    "            'true_negatives':tn, \n",
    "            'recall':recall, \n",
    "            'precision':precision,\n",
    "            'f1':f1}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 10, shuffle=True)\n",
    "c = 0\n",
    "for train_index, test_index in kf.split(df):\n",
    "    c += 1\n",
    "#     train = df[df.notary_id.isin(df.notary_id.unique()[train_index])]\n",
    "#     test = df[df.notary_id.isin(df.notary_id.unique()[test_index])]\n",
    "    train = df.loc[train_index]\n",
    "    test = df.loc[test_index]\n",
    "    \n",
    "    file = open('ranking_crossvalidation/train_files/train' + str(c) + '.txt', 'w')\n",
    "    for x in train.itertuples():\n",
    "        line = str(x.match) + ' qid:' + str(x.notary_id)\n",
    "        for y in enumerate(subcolumns):\n",
    "            if getattr(x, y[1]) != 0:\n",
    "                line = line + ' ' + str(y[0] + 1) + ':' + str(getattr(x, y[1]))\n",
    "        if x.Index + 1 == df.shape[0]:\n",
    "            file.writelines(line)\n",
    "        else:\n",
    "            file.writelines(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    file = open('ranking_crossvalidation/train_files/test' + str(c) + '.txt', 'w')\n",
    "    for x in test.itertuples():\n",
    "        line = str(x.match) + ' qid:' + str(x.notary_id)\n",
    "        for y in enumerate(subcolumns):\n",
    "            if getattr(x, y[1]) != 0:\n",
    "                line = line + ' ' + str(y[0] + 1) + ':' + str(getattr(x, y[1]))\n",
    "        if x.Index + 1 == df.shape[0]:\n",
    "            file.writelines(line)\n",
    "        else:\n",
    "            file.writelines(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    file = open('ranking_crossvalidation/train_files/vali' + str(c) + '.txt', 'w')\n",
    "    for x in test.itertuples():\n",
    "        line = str(x.match) + ' qid:' + str(x.notary_id)\n",
    "        for y in enumerate(subcolumns):\n",
    "            if getattr(x, y[1]) != 0:\n",
    "                line = line + ' ' + str(y[0] + 1) + ':' + str(getattr(x, y[1]))\n",
    "        if x.Index + 1 == df.shape[0]:\n",
    "            file.writelines(line)\n",
    "        else:\n",
    "            file.writelines(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    test.to_json('ranking_crossvalidation/train_files/test' + str(c) + '.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('ranking_crossvalidation/train_files/test1.json')\n",
    "print('Amount of matches: ' + str(len(test[test.match == 1])))\n",
    "print(test_ranker(test[test.voc_id != 'NIL'], 'ranking_crossvalidation/kfold1/export/1591783221'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('ranking_crossvalidation/train_files/test2.json')\n",
    "ranked = []\n",
    "print('Amount of matches: ' + str(len(test[test.match == 1])))\n",
    "print(test_ranker(test[test.voc_id != 'NIL'], 'ranking_crossvalidation/kfold2/export/1591782941'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('ranking_crossvalidation/train_files/test3.json')\n",
    "print('Amount of matches: ' + str(len(test[test.match == 1])))\n",
    "print(test_ranker(test[test.voc_id != 'NIL'], 'ranking_crossvalidation/kfold3/export/1591784374'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('ranking_crossvalidation/train_files/test3.json')\n",
    "print('Amount of matches: ' + str(len(test[test.match == 1])))\n",
    "print(test_ranker(test[test.voc_id != 'NIL'], 'ranking_crossvalidation/kfold3-2/export/1591784587'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('ranking_crossvalidation/train_files/test4.json')\n",
    "test = pd.read_json('ranking_crossvalidation/train_files/test3.json')\n",
    "ranked = []\n",
    "print('Amount of matches: ' + str(len(test[test.match == 1])))\n",
    "for x in range(-5, 6, 1):\n",
    "    #print('Threshold: ' +  str(x))\n",
    "    rankje = (test_ranker(test, 'ranking_crossvalidation/kfold4/export/1591179797', x))\n",
    "    ranked.append((rankje['f1'], x))\n",
    "print('Best f1: ' + str(max(ranked)[0]))\n",
    "print('Threshold: ' + str(max(ranked)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('ranking_crossvalidation/train_files/test5.json')\n",
    "for x in range(-5, 6, 1):\n",
    "    print('Threshold: ' +  str(x))\n",
    "    print('Amount of matches: ' + str(len(test[test.match == 1])))\n",
    "    print(test_ranker(test, 'ranking_crossvalidation/kfold5/export/1591180581', 0))    \n",
    "    print('_________________________________________________________________________')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('ranking_crossvalidation/train_files/test6.json')\n",
    "for x in range(-5, 6, 1):\n",
    "    print('Threshold: ' +  str(x))\n",
    "    print('Amount of matches: ' + str(len(test[test.match == 1])))\n",
    "    print(test_ranker(test, 'ranking_crossvalidation/kfold6/export/1591181359', 0))    \n",
    "    print('_________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('ranking_crossvalidation/train_files/test7.json')\n",
    "for x in range(-5, 6, 1):\n",
    "    print('Threshold: ' +  str(x))\n",
    "    print('Amount of matches: ' + str(len(test[test.match == 1])))\n",
    "    print(test_ranker(test, 'ranking_crossvalidation/kfold7/export/1591098103', 0))\n",
    "    print('_________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('ranking_crossvalidation/train_files/test8.json')\n",
    "for x in range(-5, 6, 1):\n",
    "    print('Threshold: ' +  str(x))\n",
    "    print('Amount of matches: ' + str(len(test[test.match == 1])))\n",
    "    print(test_ranker(test, 'ranking_crossvalidation/kfold8/export/1591098103', 0))\n",
    "    print('_________________________________________________________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('ranking_crossvalidation/train_files/test9.json')\n",
    "for x in range(-5, 6, 1):\n",
    "    print('Threshold: ' +  str(x))\n",
    "    print('Amount of matches: ' + str(len(test[test.match == 1])))\n",
    "    print(test_ranker(test, 'ranking_crossvalidation/kfold9/export/1591035541', 0))\n",
    "    print('_________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('ranking_crossvalidation/train_files/test10.json')\n",
    "for x in range(-5, 6, 1):\n",
    "    print('Threshold: ' +  str(x))\n",
    "    print('Amount of matches: ' + str(len(test[test.match == 1])))\n",
    "    print(test_ranker(test, 'ranking_crossvalidation/kfold10/export/1591036489', 0))\n",
    "    print('_________________________________________________________________________')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pointwise LTR_models/pointwise/export/1590928427\n",
    "Threshold: -2  \n",
    "Recall: 0.8333333333333334  \n",
    "Precision: 0.8333333333333334  \n",
    "F1: 0.8333333333333334\n",
    "\n",
    "#### Pointwise lower learning LTR_models/pointwise_lower_learning/export/1590936596\n",
    "Threshold: -1  \n",
    "Recall: 0.7777777777777778  \n",
    "Precision: 0.7777777777777778  \n",
    "F1: 0.7777777777777778\n",
    "\n",
    "#### Pointwise higher dropout LTR_models/pointwise_higher_dropout/export/1590938159\n",
    "Threshold: -2  \n",
    "Recall: 0.7222222222222222  \n",
    "Precision: 0.6842105263157895  \n",
    "F1: 0.7027027027027027\n",
    "\n",
    "#### Pointwise lower dropout LTR_models/pointwise_lower_dropout/export/1591009109\n",
    "Threshold: 0  \n",
    "Recall: 0.7692307692307693  \n",
    "Precision: 1.0  \n",
    "F1: 0.8695652173913044\n",
    "\n",
    "#### Pointwise higher learning LTR_models/pointwise_higher_learning/export/1591010427\n",
    "Threshold: -5  \n",
    "Recall: 0.9230769230769231  \n",
    "Precision: 0.75  \n",
    "F1: 0.8275862068965517\n",
    "\n",
    "#### Pointwise combined1  LTR_models/pointwise_combined1/export/1591011492\n",
    "Threshold: -1  \n",
    "Recall: 0.7692307692307693  \n",
    "Precision: 0.9090909090909091  \n",
    "F1: 0.8333333333333333\n",
    "\n",
    "#### Pairwise LTR_models/pairwise/export/1590929599\n",
    "Threshold: -1  \n",
    "Recall: 0.6111111111111112  \n",
    "Precision: 0.7857142857142857  \n",
    "F1: 0.6875000000000001\n",
    "\n",
    "#### Listwise LTR_models/listwise/export/1590930860\n",
    "Threshold: 0  \n",
    "Recall: 0.5555555555555556  \n",
    "Precision: 0.8333333333333334  \n",
    "F1: 0.6666666666666667  \n",
    "\n",
    "#### Listwise lower learning LTR_models/listwise_lower_learning/export/1590932665\n",
    "Threshold: 0  \n",
    "Recall: 0.6111111111111112  \n",
    "Precision: 0.9166666666666666  \n",
    "F1: 0.7333333333333334\n",
    "\n",
    "#### Listwise higher dropout  LTR_models/listwise_higher_dropout/export/1590934369\n",
    "Threshold: 0  \n",
    "Recall: 0.6111111111111112  \n",
    "Precision: 0.8461538461538461  \n",
    "F1: 0.7096774193548387  \n",
    "\n",
    "#### Listwise combined LTR_models/listwise_combined/export/1590935654\n",
    "Threshold: -1  \n",
    "Recall: 0.7777777777777778  \n",
    "Precision: 0.8235294117647058  \n",
    "F1: 0.7999999999999999\n",
    "\n",
    "#### Listwise lower dropout LTR_models/listwise_lower_dropout/export/1591013445\n",
    "Threshold: 0  \n",
    "Recall: 0.7222222222222222  \n",
    "Precision: 0.8666666666666667  \n",
    "F1: 0.7878787878787877\n",
    "\n",
    "#### Listwise higher learning LTR_models/listwise_higher_learning/export/1591015344\n",
    "Threshold: 0  \n",
    "Recall: 0.6666666666666666  \n",
    "Precision: 0.9230769230769231  \n",
    "F1: 0.7741935483870968\n",
    "\n",
    "#### Listwise combined2 LTR_models/listwise_combined2/export/1591016667\n",
    "Threshold: -1  \n",
    "Recall: 0.7222222222222222  \n",
    "Precision: 0.8125  \n",
    "F1: 0.7647058823529411\n",
    "\n",
    "#### Listwise combined3 LTR_models/listwise_combined3/export/1591020106\n",
    "Threshold: 0  \n",
    "Recall: 0.6666666666666666  \n",
    "Precision: 1.0  \n",
    "F1: 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code demonstrates how to use dedupe with a comma separated values\n",
    "(CSV) file. All operations are performed in memory, so will run very\n",
    "quickly on datasets up to ~10,000 rows.\n",
    "\n",
    "We start with a CSV file containing our messy data. In this example,\n",
    "it is listings of early childhood education centers in Chicago\n",
    "compiled from several different sources.\n",
    "\n",
    "The output will be a CSV with our clustered results.\n",
    "\n",
    "For larger datasets, see our [mysql_example](mysql_example.html)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "import optparse\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    # If data is missing, indicate that by setting the value to `None`\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column\n",
    "\n",
    "\n",
    "def readData(filename):\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID and each value is dict\n",
    "    \"\"\"\n",
    "\n",
    "    data_d = {}\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
    "            row_id = int(row['index'])\n",
    "            data_d[row_id] = dict(clean_row)\n",
    "    return data_d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((LevenshteinSearchPredicate: (4, name), TfidfNGramSearchPredicate: (0.6, ship_out)), (SimplePredicate: (commonThreeTokens, name), TfidfTextSearchPredicate: (0.2, ship_return)), (SimplePredicate: (sameThreeCharStartPredicate, location), SimplePredicate: (suffixArray, ship_return)), (SimplePredicate: (commonTwoTokens, ship_return), SimplePredicate: (doubleMetaphone, rank)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n",
      "reading from less_is_more2\n",
      "clustering...\n",
      "# duplicate sets 23\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "import optparse\n",
    "\n",
    "import dedupe\n",
    "from unidecode import unidecode\n",
    "\n",
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column\n",
    "\n",
    "\n",
    "def readData(filename):\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID.\n",
    "    \"\"\"\n",
    "\n",
    "    data_d = {}\n",
    "\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            clean_row = dict([(k, preProcess(v)) for (k, v) in row.items()])\n",
    "            data_d[filename + str(i)] = dict(clean_row)\n",
    "\n",
    "    return data_d\n",
    "\n",
    "# ## Setup\n",
    "\n",
    "output_file = 'data_matching_output.csv'\n",
    "settings_file = 'less_is_more2'\n",
    "training_file = 'less_is_more2.json'\n",
    "\n",
    "left_file = 'train_notary.csv'\n",
    "right_file = 'train_voc.csv'\n",
    "\n",
    "left_file = 'test_notary.csv'\n",
    "right_file = 'test_voc.csv'\n",
    "\n",
    "\n",
    "print('importing data ...')\n",
    "data_1 = readData(left_file)\n",
    "data_2 = readData(right_file)\n",
    "\n",
    "def descriptions():\n",
    "    for dataset in (data_1, data_2):\n",
    "        for record in dataset.values():\n",
    "            yield record['description']\n",
    "\n",
    "# ## Training\n",
    "\n",
    "if os.path.exists(settings_file):\n",
    "    print('reading from', settings_file)\n",
    "    with open(settings_file, 'rb') as sf:\n",
    "        linker = dedupe.StaticRecordLink(sf)\n",
    "\n",
    "else:\n",
    "    # Define the fields the linker will pay attention to\n",
    "    #\n",
    "    # Notice how we are telling the linker to use a custom field comparator\n",
    "    # for the 'price' field.\n",
    "    fields = [\n",
    "        {'field': 'name', 'type': 'String', 'has missing': True},\n",
    "        {'field': 'rank', 'type': 'String', 'has missing': True},\n",
    "        {'field': 'location', 'type': 'String', 'has missing': True},\n",
    "        {'field': 'ship_out', 'type': 'String', 'has missing': True},\n",
    "        {'field': 'ship_return', 'type': 'String', 'has missing': True}\n",
    "        ]\n",
    "\n",
    "    # Create a new linker object and pass our data model to it.\n",
    "    linker = dedupe.RecordLink(fields)\n",
    "\n",
    "    # If we have training data saved from a previous run of linker,\n",
    "    # look for it an load it in.\n",
    "    # __Note:__ if you want to train from scratch, delete the training_file\n",
    "    if os.path.exists(training_file):\n",
    "        print('reading labeled examples from ', training_file)\n",
    "        with open(training_file) as tf:\n",
    "            linker.prepare_training(data_1,\n",
    "                                    data_2,\n",
    "                                    training_file=tf,\n",
    "                                    sample_size=15000)\n",
    "    else:\n",
    "        linker.prepare_training(data_1, data_2, sample_size=15000)\n",
    "\n",
    "    # ## Active learning\n",
    "    # Dedupe will find the next pair of records\n",
    "    # it is least certain about and ask you to label them as matches\n",
    "    # or not.\n",
    "    # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "    # press 'f' when you are finished\n",
    "    print('starting active labeling...')\n",
    "\n",
    "    dedupe.console_label(linker)\n",
    "\n",
    "    linker.train()\n",
    "\n",
    "    # When finished, save our training away to disk\n",
    "    with open(training_file, 'w') as tf:\n",
    "        linker.write_training(tf)\n",
    "\n",
    "    # Save our weights and predicates to disk.  If the settings file\n",
    "    # exists, we will skip all the training and learning next time we run\n",
    "    # this file.\n",
    "    with open(settings_file, 'wb') as sf:\n",
    "        linker.write_settings(sf)\n",
    "\n",
    "# ## Blocking\n",
    "\n",
    "# ## Clustering\n",
    "\n",
    "# Find the threshold that will maximize a weighted average of our\n",
    "# precision and recall.  When we set the recall weight to 2, we are\n",
    "# saying we care twice as much about recall as we do precision.\n",
    "#\n",
    "# If we had more data, we would not pass in all the blocked data into\n",
    "# this function but a representative sample.\n",
    "\n",
    "print('clustering...')\n",
    "linked_records = linker.join(data_1, data_2, 0.0)\n",
    "\n",
    "print('# duplicate sets', len(linked_records))\n",
    "# ## Writing Results\n",
    "\n",
    "# Write our original data back out to a CSV with a new column called\n",
    "# 'Cluster ID' which indicates which records refer to each other.\n",
    "\n",
    "cluster_membership = {}\n",
    "for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "    for record_id in cluster:\n",
    "        cluster_membership[record_id] = {'Cluster ID': cluster_id,\n",
    "                                         'Link Score': score}\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "\n",
    "    header_unwritten = True\n",
    "\n",
    "    for fileno, filename in enumerate((left_file, right_file)):\n",
    "        with open(filename) as f_input:\n",
    "            reader = csv.DictReader(f_input)\n",
    "\n",
    "            if header_unwritten:\n",
    "\n",
    "                fieldnames = (['Cluster ID', 'Link Score', 'source file'] +\n",
    "                              reader.fieldnames)\n",
    "\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "\n",
    "                header_unwritten = False\n",
    "\n",
    "            for row_id, row in enumerate(reader):\n",
    "\n",
    "                record_id = filename + str(row_id)\n",
    "                cluster_details = cluster_membership.get(record_id, {})\n",
    "                row['source file'] = fileno\n",
    "                row.update(cluster_details)\n",
    "\n",
    "                writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found duplicate\n",
      "23\n",
      "precision\n",
      "0.7391304347826086\n",
      "recall\n",
      "0.68\n",
      "f1\n",
      "0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "def evaluateDuplicates(found_dupes, true_dupes):\n",
    "    true_positives = found_dupes.intersection(true_dupes)\n",
    "    false_positives = found_dupes.difference(true_dupes)\n",
    "    uncovered_dupes = true_dupes.difference(found_dupes)\n",
    "\n",
    "    print('found duplicate')\n",
    "    print(len(found_dupes))\n",
    "\n",
    "    print('precision')\n",
    "    precision = len(true_positives) / (len(true_positives) + len(false_positives))\n",
    "    print(precision)\n",
    "\n",
    "    print('recall')\n",
    "    recall = len(true_positives) / float(len(true_dupes))\n",
    "    print(recall)\n",
    "    \n",
    "    print('f1')\n",
    "    f1 = 2*((precision*recall) / (precision + recall))\n",
    "    print(f1)\n",
    "\n",
    "\n",
    "def linkPairs(filename, rowname) :\n",
    "    link_d = {}\n",
    "\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f, delimiter=',', quotechar='\"')\n",
    "        for i, row in enumerate(reader):\n",
    "            source_file, link_id = row['source file'], row[rowname]\n",
    "            if link_id:\n",
    "                if link_id not in link_d:\n",
    "                    link_d[link_id] = collections.defaultdict(list)\n",
    "\n",
    "                link_d[link_id][source_file].append(i)\n",
    "\n",
    "    link_s = set()\n",
    "\n",
    "    for members in link_d.values():\n",
    "        for pair in itertools.product(*members.values()):\n",
    "            if len(pair) > 1:\n",
    "                link_s.add(frozenset(pair))\n",
    "\n",
    "    return link_s\n",
    "\n",
    "clusters = 'data_matching_output.csv'\n",
    "\n",
    "true_dupes = linkPairs(clusters, 'cluster_id')\n",
    "test_dupes = linkPairs(clusters, 'Cluster ID')\n",
    "evaluateDuplicates(test_dupes, true_dupes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Threshold 0.0:\n",
    "#### 30 yes 30 \n",
    "precision: 0.7391304347826086  \n",
    "recall: 0.68  \n",
    "f1: 0.7083333333333334  \n",
    "\n",
    "#### 55 yes 80 no\n",
    "precision: 0.6923076923076923  \n",
    "recall: 0.72  \n",
    "f1: 0.7058823529411765  \n",
    "\n",
    "\n",
    "### Threshold 0.5\n",
    "#### 30 yes 30 \n",
    "precision: 0.8095238095238095  \n",
    "recall: 0.68  \n",
    "f1: 0.7391304347826089  \n",
    "\n",
    "#### 55 yes 80 no\n",
    "precision: 0.75  \n",
    "recall: 0.72  \n",
    "f1: 0.7346938775510204  \n",
    "\n",
    "#### 47 yes 30 no\n",
    "precision: 0.7777777777777778  \n",
    "recall: 0.56  \n",
    "f1: 0.6511627906976745  \n",
    "\n",
    "### Threshold 0.99\n",
    "#### 30 yes 30 \n",
    "precision: 0.85  \n",
    "recall: 0.68  \n",
    "f1: 0.7555555555555556  \n",
    "\n",
    "#### 55 yes 80 no\n",
    "precision: 0.7727272727272727  \n",
    "recall: 0.68  \n",
    "f1: 0.7234042553191491  \n",
    "\n",
    "\n",
    "## Nieuwe data\n",
    "\n",
    "#### 37 yes 100 no\n",
    "precision: 0.5925925925925926  \n",
    "recall: 0.64  \n",
    "f1: 0.6153846153846153  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
