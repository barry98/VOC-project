{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import notebook\n",
    "import ast\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_ranking as tfr\n",
    "from tensorflow.compat.v1 import Session\n",
    "from tensorflow.python.saved_model import loader\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook.tqdm.pandas()\n",
    "clean = pd.read_csv('../clean_data.csv')\n",
    "voc = pd.read_csv('vocop-clustered-new.csv', sep='\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid = []\n",
    "name = []\n",
    "for y, z in notebook.tqdm(clean.iterrows()):\n",
    "    for x in ast.literal_eval(z.namen):\n",
    "        if x['tussenvoegsel'] != None:\n",
    "            name.append(x['voornaam'] + \" \" + x['tussenvoegsel'] + \" \" + x['achternaam'])\n",
    "            uuid.append(z.uuid)\n",
    "        elif x['voornaam'] and x['achternaam'] != None:\n",
    "            name.append(x['voornaam'] + \" \" + x['achternaam'])\n",
    "            uuid.append(z.uuid)\n",
    "name_list = pd.DataFrame(data={'uuid':uuid, 'name':name}, columns=['uuid', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df = clean.merge(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_search(name, distance):\n",
    "    names = np.where((voc.fullNameNormalized.apply(fuzz.ratio, args=[name]) >= 90) | \n",
    "                     (voc.fullNameOriginal.dropna().apply(fuzz.ratio, args=[name]) >= 90))\n",
    "    final = (name, names)\n",
    "    return final\n",
    "\n",
    "def find_matches(names, distance):\n",
    "    name_list = {}\n",
    "    final = []\n",
    "    for x in notebook.tqdm(names):\n",
    "        if x in name_list:\n",
    "            final.append((x, name_list[x]))\n",
    "        else:\n",
    "            result = fuzzy_search(x, distance)\n",
    "            name_list[x] = result\n",
    "            final.append((x, result))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking\n",
    "\n",
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preranking.csv')\n",
    "manar = []\n",
    "menar = []\n",
    "dinar = []\n",
    "manac = []\n",
    "menac = []\n",
    "dinac = []\n",
    "maday = []\n",
    "meday = []\n",
    "diday = []\n",
    "maloc = []\n",
    "meloc = []\n",
    "diloc = []\n",
    "maran = []\n",
    "meran = []\n",
    "diran = []\n",
    "manum = []\n",
    "menum = []\n",
    "dinum = []\n",
    "for not_id in df.notary_id.unique():\n",
    "    subset = df[df.notary_id == not_id]\n",
    "    for x in subset.itertuples():\n",
    "        max_name_ratio = subset.name_ratio.max()\n",
    "        mean_name_ratio = subset.name_ratio.mean()\n",
    "        dif_name_ratio = max_name_ratio - mean_name_ratio\n",
    "        manar.append(max_name_ratio)\n",
    "        menar.append(mean_name_ratio)\n",
    "        dinar.append(dif_name_ratio)\n",
    "        \n",
    "        max_name_count = subset.name_count.max()\n",
    "        mean_name_count = subset.name_count.mean()\n",
    "        dif_name_count = max_name_count - mean_name_count\n",
    "        manac.append(max_name_count)\n",
    "        menac.append(mean_name_count)\n",
    "        dinac.append(dif_name_count)\n",
    "        \n",
    "        max_day_dif = subset.day_dif.max()\n",
    "        mean_day_dif = subset.day_dif.mean()\n",
    "        dif_day_dif = max_day_dif - mean_day_dif\n",
    "        maday.append(max_day_dif)\n",
    "        meday.append(mean_day_dif)\n",
    "        diday.append(dif_day_dif)\n",
    "        \n",
    "        max_location = subset.location.max()\n",
    "        mean_location = subset.location.mean()\n",
    "        dif_location = max_location - mean_location\n",
    "        maloc.append(max_location)\n",
    "        meloc.append(mean_location)\n",
    "        diloc.append(dif_location)\n",
    "        \n",
    "        max_rank = subset['rank'].max()\n",
    "        mean_rank = subset['rank'].mean()\n",
    "        dif_rank = max_rank - mean_rank\n",
    "        maran.append(max_rank)\n",
    "        meran.append(mean_rank)\n",
    "        diran.append(dif_rank)\n",
    "        \n",
    "        max_numships = subset.numships.max()\n",
    "        mean_numships = subset.numships.mean()\n",
    "        dif_numships = max_numships - mean_numships\n",
    "        manum.append(max_numships)\n",
    "        menum.append(mean_numships)\n",
    "        dinum.append(dif_numships)\n",
    "        \n",
    "df['max_name_ratio'] = manar\n",
    "df['mean_name_ratio'] = menar\n",
    "df['dif_name_ratio'] = dinar\n",
    "df['max_name_count'] = manac\n",
    "df['mean_name_count'] = menac\n",
    "df['dif_name_count'] = dinac\n",
    "df['max_day_dif'] = maday\n",
    "df['mean_day_dif'] = meday\n",
    "df['dif_day_dif'] = diday\n",
    "df['max_location'] = maloc\n",
    "df['mean_location'] = meloc\n",
    "df['dif_location'] = diloc\n",
    "df['max_rank'] = maran\n",
    "df['mean_rank'] = meran\n",
    "df['dif_rank'] = diran\n",
    "df['max_numships'] = manum\n",
    "df['mean_numships'] = menum\n",
    "df['dif_numships'] = dinum\n",
    "\n",
    "\n",
    "#     df2 = pd.DataFrame([[name_ratio, name_count, day_dif, location, rank, numships, match, not_id, 'NIL']],\n",
    "#                         columns= ['name_ratio', 'name_count', 'day_dif', 'location', 'rank', 'numships',\n",
    "#                                   'match', 'notary_id', 'voc_id'])\n",
    "#     df = df.append(df2, ignore_index=True)\n",
    "\n",
    "indexes = [x for x in np.random.choice(df.notary_id.unique(), int(len(df.notary_id.unique()) *0.7), replace=False)]\n",
    "train = df[df.notary_id.isin(indexes)]\n",
    "testval = df[df.notary_id.isin(indexes) == False]\n",
    "\n",
    "test_indexes = [x for x in np.random.choice(testval.notary_id.unique(), len(testval.notary_id.unique()) // 2, replace=False)]\n",
    "test = testval[testval.notary_id.isin(test_indexes)]\n",
    "val = testval[testval.notary_id.isin(test_indexes) == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('trainltr.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('testltr.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_json('valltr.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcolumns = ['name_ratio', 'name_count', 'day_dif', 'location', 'rank', 'numships',] \n",
    "#               'max_name_ratio', 'mean_name_ratio', 'dif_name_ratio',\n",
    "#               'max_name_count', 'mean_name_count', 'mean_name_count', \n",
    "#               'max_day_dif', 'mean_day_dif', 'dif_day_dif', \n",
    "#               'max_location', 'mean_location', 'dif_location', \n",
    "#               'max_rank', 'mean_rank', 'dif_rank', \n",
    "#               'max_numships', 'mean_numships', 'dif_numships']\n",
    "# subcolumns = ['name_ratio', 'day_dif', 'location', 'rank', 'numships', 'name_count', \n",
    "#               'max_name_ratio', 'mean_name_ratio', 'dif_name_ratio',\n",
    "#               'max_name_count', 'mean_name_count', 'mean_name_count', \n",
    "#               'max_day_dif', 'mean_day_dif', 'dif_day_dif', \n",
    "#               'max_location', 'mean_location', 'dif_location', \n",
    "#               'max_rank', 'mean_rank', 'dif_rank', \n",
    "#               'max_numships', 'mean_numships', 'dif_numships']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('train.txt', 'w')\n",
    "for x in train.itertuples():\n",
    "    line = str(x.match) + ' qid:' + str(x.notary_id)\n",
    "    for y in enumerate(subcolumns):\n",
    "        if getattr(x, y[1]) != 0:\n",
    "            line = line + ' ' + str(y[0] + 1) + ':' + str(getattr(x, y[1]))\n",
    "    if x.Index + 1 == df.shape[0]:\n",
    "        file.writelines(line)\n",
    "    else:\n",
    "        file.writelines(line + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('test.txt', 'w')\n",
    "for x in test.itertuples():\n",
    "    line = str(x.match) + ' qid:' + str(x.notary_id)\n",
    "    for y in enumerate(subcolumns):\n",
    "        if getattr(x, y[1]) != 0:\n",
    "            line = line + ' ' + str(y[0] + 1) + ':' + str(getattr(x, y[1]))\n",
    "    if x.Index + 1 == df.shape[0]:\n",
    "        file.writelines(line)\n",
    "    else:\n",
    "        file.writelines(line + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('vali.txt', 'w')\n",
    "for x in val.itertuples():\n",
    "    line = str(x.match) + ' qid:' + str(x.notary_id)\n",
    "    for y in enumerate(subcolumns):\n",
    "        if getattr(x, y[1]) != 0:\n",
    "            line = line + ' ' + str(y[0] + 1) + ':' + str(getattr(x, y[1]))\n",
    "    if x.Index + 1 == df.shape[0]:\n",
    "        file.writelines(line)\n",
    "    else:\n",
    "        file.writelines(line + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !saved_model_cli show \\\n",
    "#     --dir six_features/export/1590418714 \\\n",
    "#     --tag_set serve \\\n",
    "#     --signature_def predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(values, subcolumn):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    def _float_feature(value):\n",
    "        \"\"\"Returns an float_list from a int/float.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "    \n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    # data type.\n",
    "    feature = {}\n",
    "    for x in enumerate(subcolumn):\n",
    "        feature[str(x[0] + 1)] = _float_feature(values[x[1]])\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def predict(rows, id_list, directory):\n",
    "    tags=[\"serve\"]\n",
    "    signature_def_key = \"predict\"\n",
    "    saved_model_dir = directory\n",
    "    holder = {}\n",
    "    with Session() as sess:\n",
    "        loader.load(sess, tags, saved_model_dir)\n",
    "        for x in id_list:\n",
    "            values = rows[x]\n",
    "            serialized_examples = []\n",
    "            for i in range(len(values)):\n",
    "                serialized_example = serialize_example(values.iloc[i], subcolumns)\n",
    "                serialized_examples.append(serialized_example)\n",
    "            inputs_feed_dict = {'input_example_tensor:0': serialized_examples}\n",
    "            outputs = sess.run('groupwise_dnn_v2/accumulate_scores/div_no_nan:0', feed_dict=inputs_feed_dict)\n",
    "            output = [(outputs[y][0], values.iloc[y].voc_id) for y in range(len(outputs))]\n",
    "            holder[x] = output\n",
    "    return holder\n",
    "\n",
    "def test_ranker(df, directory, threshold):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn1 = 0\n",
    "    fn2 = 0\n",
    "    tn = 0\n",
    "    id_list = df.notary_id.unique()\n",
    "    holder = {}\n",
    "    for x in id_list:\n",
    "        rows = df[df.notary_id == x]\n",
    "        nil = df\n",
    "        holder[x] = rows\n",
    "    \n",
    "    ranking = predict(holder, id_list, directory)\n",
    "    \n",
    "    for x in id_list:\n",
    "        try:\n",
    "            predicted_match = max(ranking[x])\n",
    "        except:\n",
    "            for y in ranking[x]:\n",
    "                if y[1] == -1:\n",
    "                    predicted_match=y\n",
    "\n",
    "        if predicted_match[0] <= threshold:\n",
    "            if df[df.notary_id == x].match.mean() > 0:\n",
    "                fn1 += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            target = df[(df.voc_id == predicted_match[1]) & (df.notary_id == x)]\n",
    "            if target.match.iloc[0] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "                if df[df.notary_id == x].match.mean() > 0:\n",
    "                    fn2 += 1\n",
    "                    \n",
    "    recall = tp / (tp + (fn1 + fn2))\n",
    "    if tp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "    print('Recall: ' + str(recall))\n",
    "    print('Precision: ' + str(precision))\n",
    "    if precision == 0:\n",
    "        print('F1: 0')\n",
    "    else:\n",
    "        print('F1: ' + str(2*((precision*recall) / (precision + recall))))\n",
    "    return [tp, fp, fn1, fn2, tn, recall, precision]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: -5\n",
      "Recall: 0.8333333333333334\n",
      "Precision: 0.42857142857142855\n",
      "F1: 0.5660377358490566\n",
      "[15, 20, 3, 0, 89, 0.8333333333333334, 0.42857142857142855]\n",
      "_________________________________________________________________________\n",
      "Threshold: -4\n",
      "Recall: 0.8333333333333334\n",
      "Precision: 0.4838709677419355\n",
      "F1: 0.6122448979591837\n",
      "[15, 16, 3, 0, 93, 0.8333333333333334, 0.4838709677419355]\n",
      "_________________________________________________________________________\n",
      "Threshold: -3\n",
      "Recall: 0.7777777777777778\n",
      "Precision: 0.5\n",
      "F1: 0.6086956521739131\n",
      "[14, 14, 4, 0, 95, 0.7777777777777778, 0.5]\n",
      "_________________________________________________________________________\n",
      "Threshold: -2\n",
      "Recall: 0.7222222222222222\n",
      "Precision: 0.6842105263157895\n",
      "F1: 0.7027027027027027\n",
      "[13, 6, 5, 0, 103, 0.7222222222222222, 0.6842105263157895]\n",
      "_________________________________________________________________________\n",
      "Threshold: -1\n",
      "Recall: 0.6666666666666666\n",
      "Precision: 0.8\n",
      "F1: 0.7272727272727272\n",
      "[12, 3, 6, 0, 106, 0.6666666666666666, 0.8]\n",
      "_________________________________________________________________________\n",
      "Threshold: 0\n",
      "Recall: 0.5\n",
      "Precision: 1.0\n",
      "F1: 0.6666666666666666\n",
      "[9, 0, 9, 0, 109, 0.5, 1.0]\n",
      "_________________________________________________________________________\n",
      "Threshold: 1\n",
      "Recall: 0.4444444444444444\n",
      "Precision: 1.0\n",
      "F1: 0.6153846153846153\n",
      "[8, 0, 10, 0, 109, 0.4444444444444444, 1.0]\n",
      "_________________________________________________________________________\n",
      "Threshold: 2\n",
      "Recall: 0.4444444444444444\n",
      "Precision: 1.0\n",
      "F1: 0.6153846153846153\n",
      "[8, 0, 10, 0, 109, 0.4444444444444444, 1.0]\n",
      "_________________________________________________________________________\n",
      "Threshold: 3\n",
      "Recall: 0.4444444444444444\n",
      "Precision: 1.0\n",
      "F1: 0.6153846153846153\n",
      "[8, 0, 10, 0, 109, 0.4444444444444444, 1.0]\n",
      "_________________________________________________________________________\n",
      "Threshold: 4\n",
      "Recall: 0.4444444444444444\n",
      "Precision: 1.0\n",
      "F1: 0.6153846153846153\n",
      "[8, 0, 10, 0, 109, 0.4444444444444444, 1.0]\n",
      "_________________________________________________________________________\n",
      "Threshold: 5\n",
      "Recall: 0.3888888888888889\n",
      "Precision: 1.0\n",
      "F1: 0.56\n",
      "[7, 0, 11, 0, 109, 0.3888888888888889, 1.0]\n",
      "_________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for x in range(-5, 6, 1):\n",
    "    print('Threshold: ' +  str(x))\n",
    "    print(test_ranker(test, 'LTR_models/pointwise_higher_dropout/export/1590938159', x))\n",
    "    print('_________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "#### Pointwise LTR_models/pointwise/export/1590928427\n",
    "Threshold: -2  \n",
    "Recall: 0.8333333333333334  \n",
    "Precision: 0.8333333333333334  \n",
    "F1: 0.8333333333333334\n",
    "\n",
    "#### Pointwise lower learning LTR_models/pointwise_lower_learning/export/1590936596\n",
    "Threshold: -1  \n",
    "Recall: 0.7777777777777778  \n",
    "Precision: 0.7777777777777778  \n",
    "F1: 0.7777777777777778\n",
    "\n",
    "#### Pointwise higher dropout LTR_models/pointwise_higher_dropout/export/1590938159\n",
    "Threshold: -2  \n",
    "Recall: 0.7222222222222222  \n",
    "Precision: 0.6842105263157895  \n",
    "F1: 0.7027027027027027\n",
    "\n",
    "#### Pairwise LTR_models/pairwise/export/1590929599\n",
    "Threshold: -1  \n",
    "Recall: 0.6111111111111112  \n",
    "Precision: 0.7857142857142857  \n",
    "F1: 0.6875000000000001\n",
    "\n",
    "#### Listwise LTR_models/listwise/export/1590930860\n",
    "Threshold: 0  \n",
    "Recall: 0.5555555555555556  \n",
    "Precision: 0.8333333333333334  \n",
    "F1: 0.6666666666666667  \n",
    "\n",
    "#### Listwise lower learning LTR_models/listwise_lower_learning/export/1590932665\n",
    "Threshold: 0  \n",
    "Recall: 0.6111111111111112  \n",
    "Precision: 0.9166666666666666  \n",
    "F1: 0.7333333333333334\n",
    "\n",
    "#### Listwise higher dropout  LTR_models/listwise_higher_dropout/export/1590934369\n",
    "Threshold: 0  \n",
    "Recall: 0.6111111111111112  \n",
    "Precision: 0.8461538461538461  \n",
    "F1: 0.7096774193548387  \n",
    "\n",
    "#### Listwise combined LTR_models/listwise_combined/export/1590935654\n",
    "Threshold: -1\n",
    "Recall: 0.7777777777777778\n",
    "Precision: 0.8235294117647058\n",
    "F1: 0.7999999999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
