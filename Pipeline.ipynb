{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\tqdm\\std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import notebook\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import timeit\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import Session\n",
    "from tensorflow.python.saved_model import loader\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "import optparse\n",
    "import dedupe\n",
    "from unidecode import unidecode\n",
    "notebook.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (17,27,35,36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load('../Spacy models/kfold_3', disable=['parser', 'tagger', 'textcat'])\n",
    "notary = pd.read_csv('../clean_data.csv')\n",
    "voc = pd.read_csv('vocop_clustered_dutchrank.csv')\n",
    "#voc = pd.read_csv('vocop-clustered-new.csv', sep='\t')\n",
    "#rangen = pd.read_excel('../vocop_rangen.xlsx', index_col=0)\n",
    "#voc['dutch_rank'] = [translate_rank(x, rangen) for x in notebook.tqdm(voc['rank'].tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER(text, nlp):\n",
    "    ''' Takes in a string and uses the specified NLP model to tag entities within the string.\n",
    "        Returns all entities tagged as PERSON within the string.\n",
    "    '''\n",
    "    # Tag text\n",
    "    doc = nlp(text)\n",
    "    holder = []\n",
    "    \n",
    "    # Append all entities tagged as PERSON\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON' and ' ' in ent.text:\n",
    "            holder.append(ent.text)\n",
    "    return holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(model_dir, notary, voc):\n",
    "    settings_file = model_dir\n",
    "\n",
    "\n",
    "    print('importing data ...')\n",
    "    data_1 = notary.to_dict(orient='index')\n",
    "    data_2 = voc.to_dict(orient='index')\n",
    "\n",
    "    def descriptions():\n",
    "        for dataset in (data_1, data_2):\n",
    "            for record in dataset.values():\n",
    "                yield record['description']\n",
    "\n",
    "    # ## Training\n",
    "\n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            linker = dedupe.StaticRecordLink(sf)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Could not find model')\n",
    "    # ## Blocking\n",
    "\n",
    "    # ## Clustering\n",
    "\n",
    "    # Find the threshold that will maximize a weighted average of our\n",
    "    # precision and recall.  When we set the recall weight to 2, we are\n",
    "    # saying we care twice as much about recall as we do precision.\n",
    "    #\n",
    "    # If we had more data, we would not pass in all the blocked data into\n",
    "    # this function but a representative sample.\n",
    "    f1 = []\n",
    "    tests = []\n",
    "    print('clustering...')\n",
    "    try:\n",
    "        linked_records = linker.join(data_1, data_2, 0.1)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    print('# duplicate sets', len(linked_records))\n",
    "    # ## Writing Results\n",
    "\n",
    "    # Write our original data back out to a CSV with a new column called\n",
    "    # 'Cluster ID' which indicates which records refer to each other.\n",
    "\n",
    "    cluster_membership = {}\n",
    "    for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "        for record_id in cluster:\n",
    "            cluster_membership[record_id] = {'Cluster ID': cluster_id,\n",
    "                                             'Link Score': score}\n",
    "    clusters = {}\n",
    "    for x in cluster_membership:\n",
    "        if cluster_membership[x]['Cluster ID'] not in clusters:\n",
    "            clusters[cluster_membership[x]['Cluster ID']] = x\n",
    "        else:\n",
    "            clusters[cluster_membership[x]['Cluster ID']] = [clusters[cluster_membership[x]['Cluster ID']], x]\n",
    "    \n",
    "    matches = [clusters[x] for x in clusters] \n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_neighbour(start, end, true, prev, distance):\n",
    "    if true == []:\n",
    "        return (start, end), prev.i\n",
    "    if prev.i == len(prev.doc) - 1:\n",
    "        return (start, end), prev.i\n",
    "    if fuzz.ratio(true[0].lower(), prev.nbor().text.lower()) >= distance:\n",
    "        return match_neighbour(start, prev.nbor().idx + len(prev.nbor()), true[1:], prev.nbor(), distance)\n",
    "    else:\n",
    "        return (start, end), prev.i\n",
    "\n",
    "def match_finder(row, match, distance, nlp):\n",
    "\n",
    "    true = match\n",
    "    doc = nlp(row.text)\n",
    "\n",
    "    locs = []\n",
    "    prev = 0\n",
    "    for token in doc:\n",
    "        for x in true:\n",
    "            if token.i > prev and type(x) == str:\n",
    "                split = x.split(' ')\n",
    "                if fuzz.ratio(split[0].lower(), token.text.lower()) >= distance:\n",
    "                    result, prev = match_neighbour(token.idx, token.idx + len(token), split[1:], token, distance)\n",
    "                    if result not in locs:\n",
    "                        if fuzz.ratio(row.text[result[0]:result[1]], x) >= distance:\n",
    "                            locs.append(result)\n",
    "                    elif len(split) > 1 and fuzz.ratio(split[1].lower(), token.text.lower()) >= distance:\n",
    "                        result, prev = match_neighbour(token.nbor(-1).idx, token.idx + len(token), split[1:], token, distance)\n",
    "                        if result not in locs:\n",
    "                            if fuzz.ratio(row.text[result[0]:result[1]], x) >= distance:\n",
    "                                locs.append(result)\n",
    "    entities = [row.text[x[0]:x[1]] for x in locs]\n",
    "    return entities\n",
    "\n",
    "def find_entities(row, x, column, nlp):\n",
    "    if type(x[column]) != float: \n",
    "        match = match_finder(row, [x[column]], 80, nlp) \n",
    "    else:\n",
    "        match = ''\n",
    "    return match\n",
    "\n",
    "def find_matches(name, knowledgebase, target_column, distance=90):\n",
    "    ''' Takes in a string containing the name of a person and returns all possible matches from the\n",
    "        knowledgebase based on fuzzy string matching.\n",
    "    '''\n",
    "    matches = knowledgebase[(knowledgebase[target_column].str.lower().astype(str).apply(fuzz.ratio, args=[name.lower()]) >= distance)]\n",
    "    return matches\n",
    "\n",
    "def convert_matches(entity, matches, knowledgebase, nlp):\n",
    "    data_n = {}\n",
    "    data_v = {}\n",
    "    row = entity\n",
    "    holder = []\n",
    "    notary_date = datetime.strptime(row.datering, '%Y-%m-%d')\n",
    "    \n",
    "    for x in range(matches.shape[0]):\n",
    "        try:\n",
    "            date1 = datetime.strptime(matches['date_begin_service_complete'].iloc[x], '%Y-%m-%d')\n",
    "        except:\n",
    "            date1 = datetime(year=1, month=1, day =1 )\n",
    "        try:\n",
    "            date2 = datetime.strptime(matches['date_end_service_complete'].iloc[x], '%Y-%m-%d')\n",
    "        except:\n",
    "            date2 = datetime(year=1, month=1, day =1 )\n",
    "        if abs((notary_date - date1).days) < 90 or abs((notary_date - date2).days) < 90:\n",
    "            found_ship_out = find_entities(row, matches.iloc[x], 'shipOutward', nlp)\n",
    "            found_ship_return = find_entities(row, matches.iloc[x], 'shipReturn', nlp)\n",
    "            found_loc = find_entities(row, matches.iloc[x], 'placeOfOrigin', nlp)\n",
    "            found_rank = find_entities(row, matches.iloc[x], 'dutch_rank', nlp)\n",
    "            holder.append({'rank':found_rank, 'location':found_loc, 'found_ship_return': found_ship_return, 'found_ship_out': found_ship_out})\n",
    "\n",
    "            cluster = int(matches.iloc[x].VOCOP_id)\n",
    "            name = str(matches.iloc[x].fullNameOriginal)\n",
    "            rang = str(matches.iloc[x].dutch_rank)\n",
    "            loc = str(matches.iloc[x].placeOfOrigin)\n",
    "            ship_out = str(matches.iloc[x].shipOutward) \n",
    "            ship_return = str(matches.iloc[x].shipReturn)\n",
    "            data_v[cluster] = {'name':name, 'rank':rang, 'location':loc, 'ship_out': ship_out, 'ship_return': ship_return}\n",
    "    \n",
    "    name = row['name']\n",
    "    if holder == []:\n",
    "        rang = None\n",
    "        location = None\n",
    "        ship_out= None\n",
    "        ship_return = None\n",
    "    else:\n",
    "        rang = ' | '.join(set([y.lower() for x in holder for y in x['rank']]))\n",
    "        if rang == '':\n",
    "            rang = None\n",
    "        location = ' | '.join(set([y.lower() for x in holder for y in x['location']]))\n",
    "        if location == '':\n",
    "            location = None\n",
    "        ship_out = ' | '.join(set([y.lower() for x in holder for y in x['found_ship_out']]))\n",
    "        if ship_out == '':\n",
    "            ship_return = None\n",
    "        ship_return = ' | '.join(set([y.lower() for x in holder for y in x['found_ship_return']]))\n",
    "        if ship_return == '':\n",
    "            ship_return = None\n",
    "    data_n[entity.uuid] = {'name':name, 'rank':rang, 'location':location, 'ship_out': ship_out, 'ship_return': ship_return}\n",
    "\n",
    "    d_n = {'index': [x for x in data_n], 'name': [data_n[x]['name'] for x in data_n], 'rank':[data_n[x]['rank'] for x in data_n],\n",
    "         'location':[data_n[x]['location'] for x in data_n], 'ship_out': [data_n[x]['ship_out'] for x in data_n], 'ship_return': [data_n[x]['ship_return'] for x in data_n]}\n",
    "\n",
    "    d_v = {'index': [x for x in data_v], 'name': [data_v[x]['name'] for x in data_v], 'rank':[data_v[x]['rank'] for x in data_v],\n",
    "         'location':[data_v[x]['location'] for x in data_v], 'ship_out': [data_v[x]['ship_out'] for x in data_v], 'ship_return': [data_v[x]['ship_return'] for x in data_v]}\n",
    "\n",
    "    dedupe_notary = pd.DataFrame(d_n).set_index('index')\n",
    "    dedupe_voc = pd.DataFrame(d_v).set_index('index')\n",
    "    return dedupe_notary, dedupe_voc\n",
    "\n",
    "def RL(entity, knowledgebase, model_dir, nlp):\n",
    "    ''' Takes in a string containing an entity and returns either a match within the specified pandas \n",
    "        knowledgebase or None if no match is present.\n",
    "    ''' \n",
    "    # Find and narrow down matches\n",
    "    possible_matches = find_matches(entity['name'], knowledgebase, 'fullNameOriginal')\n",
    "    converted_entities, converted_matches = convert_matches(entity, possible_matches, knowledgebase, nlp)\n",
    "    clusters = clustering(model_dir, converted_entities, converted_matches)\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pipeline(row, knowledgebase, NER_model, RL_model):\n",
    "    holder = []\n",
    "    entities = NER(row.text, NER_model)\n",
    "    for x in entities:\n",
    "        row['name'] = x\n",
    "        holder.append(RL(row, knowledgebase, RL_model, NER_model))\n",
    "    return holder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "INFO:dedupe.api:((TfidfNGramSearchPredicate: (0.2, name), TfidfNGramSearchPredicate: (0.6, ship_out)), (ExistsPredicate: (Exists, ship_out), SimplePredicate: (commonThreeTokens, name)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n",
      "reading from rl models/1003no\n",
      "clustering...\n",
      "# duplicate sets 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((TfidfNGramSearchPredicate: (0.2, name), TfidfNGramSearchPredicate: (0.6, ship_out)), (ExistsPredicate: (Exists, ship_out), SimplePredicate: (commonThreeTokens, name)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n",
      "reading from rl models/1003no\n",
      "clustering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:((TfidfNGramSearchPredicate: (0.2, name), TfidfNGramSearchPredicate: (0.6, ship_out)), (ExistsPredicate: (Exists, ship_out), SimplePredicate: (commonThreeTokens, name)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n",
      "reading from rl models/1003no\n",
      "clustering...\n",
      "Seconds for single query: 15.787028400000054\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "x = Pipeline(notary.iloc[85], voc, nlp2, 'rl models/1003no')\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('Seconds for single query: ' + str(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['ee21c8bc-bfea-25ee-c6c1-ddf74644012b', '802002']], None, None]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NEL(entity, knowledgebase, model_dir):\n",
    "    ''' Takes in a string containing an entity and returns either a match within the specified pandas \n",
    "        knowledgebase or None if no match is present.\n",
    "    ''' \n",
    "    # Find and narrow down matches\n",
    "    possible_matches = find_matches(entity.naam, knowledgebase, 'fullNameOriginal')\n",
    "    name_dict = knowledgebase.fullNameNormalized.value_counts()\n",
    "    converted_matches = convert_matches(entity, possible_matches, name_dict)\n",
    "    if converted_matches == {}:\n",
    "        return None\n",
    "    ranked_matches = ranking(converted_matches, model_dir)\n",
    "    return max(ranked_matches)\n",
    "\n",
    "def convert_matches(entity, matches, name_dict):\n",
    "    \n",
    "    holder = {}\n",
    "    notary_date = datetime.strptime(entity.datering, '%Y-%m-%d')\n",
    "    \n",
    "    # Try to convert the dates for each entry into datetime format\n",
    "    for x in range(matches.shape[0]):\n",
    "        try:\n",
    "            date1 = datetime.strptime(matches['date_begin_service_complete'].iloc[x], '%Y-%m-%d')\n",
    "        except:\n",
    "            date1 = datetime(year=1, month=1, day =1 )\n",
    "        try:\n",
    "            date2 = datetime.strptime(matches['date_end_service_complete'].iloc[x], '%Y-%m-%d')\n",
    "        except:\n",
    "            date2 = datetime(year=1, month=1, day =1 )\n",
    "            \n",
    "        # Keep only matches that are within distance days from notary_date\n",
    "        if abs((notary_date - date1).days) < 90 or abs((notary_date - date2).days) < 90:\n",
    "            name_ratio = fuzz.ratio(entity.naam, matches['fullNameOriginal'].iloc[x])\n",
    "            name_count = name_dict[matches['fullNameNormalized'].iloc[x]]\n",
    "            vocid=matches['VOCOP_id'].iloc[x]\n",
    "            day_dif = min([abs((notary_date - date1).days), abs((notary_date - date2).days)])\n",
    "            location = len(match_finder(entity.text, [matches['placeOfOrigin'].iloc[x]], 80))\n",
    "            rank = len(match_finder(entity.text, [matches['dutch_rank'].iloc[x]], 80))\n",
    "            numships = len(match_finder(entity.text, [matches['shipOutward'].iloc[x], matches['shipReturn'].iloc[x]], 80))\n",
    "            holder[vocid] = [name_ratio, name_count, day_dif, location, rank, numships]\n",
    "\n",
    "    return holder\n",
    "\n",
    "def serialize_example(values):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    def _float_feature(value):\n",
    "        \"\"\"Returns an float_list from a int/float.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "    \n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    # data type.\n",
    "    feature = {}\n",
    "    for x in enumerate(values):\n",
    "        feature[str(x[0] + 1)] = _float_feature(values[x[0]])\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def ranking(matches, directory):\n",
    "    tags=[\"serve\"]\n",
    "    signature_def_key = \"predict\"\n",
    "    saved_model_dir = directory\n",
    "    holder = []\n",
    "    with Session() as sess:\n",
    "        loader.load(sess, tags, saved_model_dir)\n",
    "        serialized_examples = []\n",
    "        vocop_ids = [x for x in matches]\n",
    "        for x in matches:\n",
    "            names = matches[x][0]\n",
    "            days = matches[x][2]\n",
    "            locations = matches[x][3]\n",
    "            ranks = matches[x][4]\n",
    "            ships = matches[x][5]\n",
    "            name_count = matches[x][1]\n",
    "            serialized_example = serialize_example([names, name_count, days, locations, \n",
    "                                                   ranks, ships] )\n",
    "            serialized_examples.append(serialized_example)\n",
    "            inputs_feed_dict = {'input_example_tensor:0': serialized_examples}\n",
    "            outputs = sess.run('groupwise_dnn_v2/accumulate_scores/div_no_nan:0', feed_dict=inputs_feed_dict)\n",
    "            output = [(outputs[x], vocop_ids[x]) for x in range(len(outputs))]\n",
    "            holder = output\n",
    "    return holder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
