{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\tqdm\\std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import notebook\n",
    "from datetime import datetime\n",
    "notebook.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (17,27,35,36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load('../Spacy models/kfold_3', disable=['parser', 'tagger', 'textcat'])\n",
    "voc = pd.read_csv('vocop-clustered-new.csv', sep='\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER(text, nlp):\n",
    "    ''' Takes in a string and uses the specified NLP model to tag entities within the string.\n",
    "        Returns all entities tagged as PERSON within the string.\n",
    "    '''\n",
    "    # Tag text\n",
    "    doc = nlp(text)\n",
    "    holder = []\n",
    "    \n",
    "    # Append all entities tagged as PERSON\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON' and ' ' in ent.text:\n",
    "            holder.append(ent.text)\n",
    "    return holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "        \"\"\"Returns an float_list from a int/float.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def serialize_example(features):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible data type.\n",
    "    feature = {}\n",
    "    for x in enumerate(features):\n",
    "        feature[str(x[0] + 1)] = _float_feature(x[1])\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def ranking(matches, directory):\n",
    "    tags=[\"serve\"]\n",
    "    signature_def_key = \"predict\"\n",
    "    saved_model_dir = directory\n",
    "    holder = {}\n",
    "    with Session() as sess:\n",
    "        loader.load(sess, tags, saved_model_dir)\n",
    "        serialized_examples = []\n",
    "        for x in matches:\n",
    "            vocop_ids = x\n",
    "            names = matches[x][0]\n",
    "            days = matches[x][1]\n",
    "            locations = matches[x][2]\n",
    "            ranks = matches[x][3]\n",
    "            ships = matches[x][4]\n",
    "            name_count = matches[x][5]\n",
    "            serialized_example = serialize_example(names, days, locations, \n",
    "                                                   ranks, ships, name_count)\n",
    "            serialized_examples.append(serialized_example)\n",
    "        inputs_feed_dict = {'input_example_tensor:0': serialized_examples}\n",
    "        outputs = sess.run('groupwise_dnn_v2/accumulate_scores/div_no_nan:0', feed_dict=inputs_feed_dict)\n",
    "        output = [(outputs[x][0], vocop_ids[x]) for x in range(len(outputs))]\n",
    "        holder[x] = output\n",
    "    return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(name, knowledgebase, target_column, distance=90):\n",
    "    ''' Takes in a string containing the name of a person and returns all possible matches from the\n",
    "        knowledgebase based on fuzzy string matching.\n",
    "    '''\n",
    "    matches = knowledgebase[(knowledgebase[target_column].astype(str).apply(fuzz.ratio, args=[name]) >= distance)]\n",
    "    return matches\n",
    "\n",
    "def convert_matches(entity, matches, name_dict):\n",
    "    \n",
    "    holder = {}\n",
    "    notary_date = datetime.strptime(entity, '%Y-%m-%d')\n",
    "    \n",
    "    # Try to convert the dates for each entry into datetime format\n",
    "    for x in range(matches.shape[0]):\n",
    "        try:\n",
    "            date1 = datetime.strptime(matches['date_begin_service_complete'].iloc[x], '%Y-%m-%d')\n",
    "        except:\n",
    "            date1 = datetime(year=1, month=1, day =1 )\n",
    "        try:\n",
    "            date2 = datetime.strptime(matches['date_end_service_complete'].iloc[x], '%Y-%m-%d')\n",
    "        except:\n",
    "            date2 = datetime(year=1, month=1, day =1 )\n",
    "            \n",
    "        # Keep only matches that are within distance days from notary_date\n",
    "        if abs((notary_date - date1).days) < distance or abs((notary_date - date2).days) < distance:\n",
    "            name_ratio = fuzz.ratio(entity, matches['fullNameOriginal'])\n",
    "            name_count = name_dict[matches['fullNameNormalized']]\n",
    "            vocid=matches['VOCOP_id']\n",
    "            day_dif = min([abs((notary_date - date1).days), abs((notary_date - date2).days)])\n",
    "            location = \n",
    "            rank =\n",
    "            numships = \n",
    "            holder[vocid] = [name_ratio, day_dif, location, rank, numships, name_count]\n",
    "\n",
    "    return holder\n",
    "\n",
    "def NEL(entity, knowledgebase, model_dir):\n",
    "    ''' Takes in a string containing an entity and returns either a match within the specified pandas \n",
    "        knowledgebase or None if no match is present.\n",
    "    ''' \n",
    "    # Find and narrow down matches\n",
    "    possible_matches = find_matches(entity, knowledgebase, 'fullNameOriginal')\n",
    "    name_dict = knowledgebase.fullNameNormalized.value_counts()\n",
    "    converted_matches = convert_matches(entity, possible_matches, name_dict)\n",
    "    \n",
    "    if converted_matches == []:\n",
    "        return None\n",
    "    ranked_matches = ranking(converted_matches, model_dir)\n",
    "    return ranked_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = NEL('Jan de Vrij', '1751-08-19', voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol\n",
      "sick\n"
     ]
    }
   ],
   "source": [
    "for x in {'lol': 12, 'sick':13}:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
